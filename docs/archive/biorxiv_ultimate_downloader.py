#!/usr/bin/env python3
"""
ğŸš€ BioRxivç»ˆæä¸‹è½½å™¨
ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„åçˆ¬è™«ç»•è¿‡æŠ€æœ¯
"""

import asyncio
import logging
import tempfile
import os
import time
import requests
import json
from typing import Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class BioRxivDownloader:
    """BioRxivä¸“ç”¨ä¸‹è½½å™¨ç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def method_1_smart_http(self, pdf_url: str) -> Optional[bytes]:
        """æ–¹æ³•1: æ™ºèƒ½HTTPè¯·æ±‚ï¼ˆå¤šæ­¥é¢„çƒ­ï¼‰"""
        try:
            logger.info("ğŸ¯ æ–¹æ³•1: æ™ºèƒ½HTTPè¯·æ±‚")
            
            # æ­¥éª¤1: è®¿é—®bioRxivä¸»é¡µ
            logger.info("  1.1 è®¿é—®bioRxivä¸»é¡µ...")
            self.session.get('https://www.biorxiv.org/', timeout=10)
            time.sleep(0.5)
            
            # æ­¥éª¤2: è®¿é—®è®ºæ–‡é¡µé¢
            paper_url = pdf_url.replace('.full.pdf', '').replace('/pdf/', '/content/')
            logger.info(f"  1.2 è®¿é—®è®ºæ–‡é¡µé¢: {paper_url}")
            
            paper_resp = self.session.get(paper_url, timeout=15)
            if paper_resp.status_code != 200:
                logger.warning(f"    è®ºæ–‡é¡µé¢è®¿é—®å¤±è´¥: {paper_resp.status_code}")
                return None
            
            time.sleep(1)  # æ¨¡æ‹Ÿäººç±»é˜…è¯»æ—¶é—´
            
            # æ­¥éª¤3: è®¿é—®PDF URL
            logger.info(f"  1.3 è®¿é—®PDF URL: {pdf_url}")
            pdf_headers = self.session.headers.copy()
            pdf_headers.update({
                'Accept': 'application/pdf,*/*',
                'Referer': paper_url,
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin'
            })
            
            pdf_resp = self.session.get(pdf_url, headers=pdf_headers, timeout=30)
            
            if pdf_resp.status_code == 200:
                content = pdf_resp.content
                if content and content.startswith(b'%PDF'):
                    logger.info(f"  âœ… æ™ºèƒ½HTTPæˆåŠŸ: {len(content)} bytes")
                    return content
                else:
                    logger.warning("  âŒ è¿”å›å†…å®¹ä¸æ˜¯PDF")
            else:
                logger.warning(f"  âŒ PDFè¯·æ±‚å¤±è´¥: {pdf_resp.status_code}")
            
            return None
            
        except Exception as e:
            logger.warning(f"  âŒ æ™ºèƒ½HTTPå¼‚å¸¸: {e}")
            return None
    
    async def method_2_playwright_stealth(self, pdf_url: str) -> Optional[bytes]:
        """æ–¹æ³•2: è¶…çº§éšèº«æµè§ˆå™¨"""
        try:
            from playwright.async_api import async_playwright
            
            logger.info("ğŸ¥· æ–¹æ³•2: è¶…çº§éšèº«æµè§ˆå™¨")
            
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-accelerated-2d-canvas',
                    '--no-first-run',
                    '--no-zygote',
                    '--single-process',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor,TranslateUI',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-images',  # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼Œæé€Ÿ
                    '--disable-javascript',  # ç¦ç”¨JavaScriptï¼Œé¿å…æ£€æµ‹
                    '--user-data-dir=/tmp/chrome_user_data'
                ]
            )
            
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache'
                }
            )
            
            page = await context.new_page()
            
            # è®¾ç½®ä¸‹è½½å¤„ç†
            download_info = {'completed': False, 'content': None}
            
            async def handle_download(download):
                try:
                    temp_path = tempfile.mktemp(suffix='.pdf')
                    await download.save_as(temp_path)
                    
                    with open(temp_path, 'rb') as f:
                        content = f.read()
                    
                    os.unlink(temp_path)
                    
                    if content and content.startswith(b'%PDF'):
                        download_info['content'] = content
                        download_info['completed'] = True
                        logger.info(f"  âœ… æµè§ˆå™¨ä¸‹è½½æˆåŠŸ: {len(content)} bytes")
                except Exception as e:
                    logger.error(f"  âŒ ä¸‹è½½å¤„ç†å¼‚å¸¸: {e}")
            
            page.on("download", handle_download)
            
            # å¤šé˜¶æ®µå¯¼èˆª
            try:
                logger.info("  2.1 è®¿é—®bioRxivä¸»é¡µ...")
                await page.goto('https://www.biorxiv.org/', timeout=20000)
                await asyncio.sleep(1)
                
                paper_url = pdf_url.replace('.full.pdf', '').replace('/pdf/', '/content/')
                logger.info("  2.2 è®¿é—®è®ºæ–‡é¡µé¢...")
                await page.goto(paper_url, timeout=20000)
                await asyncio.sleep(2)
                
                logger.info("  2.3 ç›´æ¥è®¿é—®PDF...")
                await page.goto(pdf_url, timeout=30000)
            except Exception as e:
                logger.info(f"  2.x å¯¼èˆªå¼‚å¸¸ï¼ˆå¯èƒ½æ­£å¸¸ï¼‰: {e}")
            
            # ç­‰å¾…ä¸‹è½½
            for i in range(100):  # 10ç§’
                if download_info['completed']:
                    break
                await asyncio.sleep(0.1)
            
            await context.close()
            await browser.close()
            await playwright.stop()
            
            return download_info.get('content')
            
        except Exception as e:
            logger.warning(f"  âŒ éšèº«æµè§ˆå™¨å¼‚å¸¸: {e}")
            return None
    
    async def method_3_curl_simulation(self, pdf_url: str) -> Optional[bytes]:
        """æ–¹æ³•3: æ¨¡æ‹Ÿcurlè¯·æ±‚"""
        try:
            logger.info("ğŸŒ æ–¹æ³•3: æ¨¡æ‹Ÿcurlè¯·æ±‚")
            
            import subprocess
            
            # ä½¿ç”¨curlæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¯·æ±‚
            paper_url = pdf_url.replace('.full.pdf', '').replace('/pdf/', '/content/')
            
            # ç¬¬ä¸€æ­¥ï¼šè·å–è®ºæ–‡é¡µé¢çš„cookies
            logger.info("  3.1 ä½¿ç”¨curlè·å–cookies...")
            curl_cmd1 = [
                'curl', '-s', '-L',
                '-H', 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                '-H', 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                '-H', 'Accept-Language: en-US,en;q=0.5',
                '-H', 'Accept-Encoding: gzip, deflate',
                '-H', 'Connection: keep-alive',
                '-H', 'Upgrade-Insecure-Requests: 1',
                '-c', '/tmp/biorxiv_cookies.txt',
                paper_url
            ]
            
            subprocess.run(curl_cmd1, capture_output=True, timeout=15)
            
            # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨cookiesä¸‹è½½PDF
            logger.info("  3.2 ä½¿ç”¨cookiesä¸‹è½½PDF...")
            curl_cmd2 = [
                'curl', '-s', '-L',
                '-H', 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                '-H', 'Accept: application/pdf,*/*',
                '-H', f'Referer: {paper_url}',
                '-H', 'Connection: keep-alive',
                '-b', '/tmp/biorxiv_cookies.txt',
                '-o', '/tmp/biorxiv_curl.pdf',
                pdf_url
            ]
            
            result = subprocess.run(curl_cmd2, capture_output=True, timeout=30)
            
            if result.returncode == 0 and os.path.exists('/tmp/biorxiv_curl.pdf'):
                with open('/tmp/biorxiv_curl.pdf', 'rb') as f:
                    content = f.read()
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink('/tmp/biorxiv_curl.pdf')
                    os.unlink('/tmp/biorxiv_cookies.txt')
                except:
                    pass
                
                if content and content.startswith(b'%PDF'):
                    logger.info(f"  âœ… curlä¸‹è½½æˆåŠŸ: {len(content)} bytes")
                    return content
            
            return None
            
        except Exception as e:
            logger.warning(f"  âŒ curlæ¨¡æ‹Ÿå¼‚å¸¸: {e}")
            return None
    
    def method_4_proxy_rotation(self, pdf_url: str) -> Optional[bytes]:
        """æ–¹æ³•4: ä»£ç†è½®æ¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        try:
            logger.info("ğŸ”„ æ–¹æ³•4: ä»£ç†è½®æ¢")
            
            # ä¸€äº›å…è´¹ä»£ç†ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”è¯¥ç”¨æ›´å¥½çš„ä»£ç†æœåŠ¡ï¼‰
            proxies_list = [
                None,  # ç›´è¿
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ä»£ç†æœåŠ¡å™¨
            ]
            
            for i, proxy in enumerate(proxies_list):
                try:
                    logger.info(f"  4.{i+1} å°è¯•ä»£ç†: {proxy or 'ç›´è¿'}")
                    
                    session = requests.Session()
                    if proxy:
                        session.proxies.update({'http': proxy, 'https': proxy})
                    
                    session.headers.update({
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                    })
                    
                    # å¿«é€Ÿå°è¯•
                    resp = session.get(pdf_url, timeout=10)
                    if resp.status_code == 200 and resp.content.startswith(b'%PDF'):
                        logger.info(f"  âœ… ä»£ç†æˆåŠŸ: {len(resp.content)} bytes")
                        return resp.content
                        
                except Exception as e:
                    logger.info(f"  âŒ ä»£ç†{i+1}å¤±è´¥: {e}")
                    continue
            
            return None
            
        except Exception as e:
            logger.warning(f"  âŒ ä»£ç†è½®æ¢å¼‚å¸¸: {e}")
            return None


def download_biorxiv_pdf(pdf_url: str) -> Optional[bytes]:
    """ä¸»å…¥å£ï¼šç»ˆæbioRxivä¸‹è½½å™¨"""
    logger.info(f"ğŸš€ å¯åŠ¨ç»ˆæbioRxivä¸‹è½½å™¨: {pdf_url}")
    
    downloader = BioRxivDownloader()
    methods = [
        ("æ™ºèƒ½HTTP", downloader.method_1_smart_http),
        # ğŸ”§ æš‚æ—¶ç¦ç”¨å¼‚æ­¥æ–¹æ³•ï¼Œé¿å…event loopå†²çª
        # ("éšèº«æµè§ˆå™¨", lambda url: asyncio.run(downloader.method_2_playwright_stealth(url))),
        # ("curlæ¨¡æ‹Ÿ", lambda url: asyncio.run(downloader.method_3_curl_simulation(url))),
        ("ä»£ç†è½®æ¢", downloader.method_4_proxy_rotation),
    ]
    
    for method_name, method_func in methods:
        try:
            logger.info(f"\nğŸ”„ å°è¯•æ–¹æ³•: {method_name}")
            content = method_func(pdf_url)
            
            if content:
                logger.info(f"ğŸ‰ {method_name}æˆåŠŸï¼")
                return content
            else:
                logger.info(f"âš ï¸ {method_name}å¤±è´¥")
                
        except Exception as e:
            logger.warning(f"âŒ {method_name}å¼‚å¸¸: {e}")
            continue
    
    logger.error("ğŸ’¥ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼bioRxivçš„åçˆ¬è™«å¤ªå¼ºäº†")
    return None


if __name__ == "__main__":
    # æµ‹è¯•
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
    
    test_url = "https://www.biorxiv.org/content/10.1101/2025.09.21.677607v1.full.pdf"
    
    print(f"ğŸ§ª æµ‹è¯•ç»ˆæä¸‹è½½å™¨: {test_url}")
    print("=" * 60)
    
    content = download_biorxiv_pdf(test_url)
    
    if content:
        print(f"\nâœ… æˆåŠŸï¼ä¸‹è½½äº† {len(content)} bytes")
        with open('/tmp/test_biorxiv_ultimate.pdf', 'wb') as f:
            f.write(content)
        print("ğŸ“ æµ‹è¯•æ–‡ä»¶å·²ä¿å­˜åˆ° /tmp/test_biorxiv_ultimate.pdf")
    else:
        print("\nâŒ ç»ˆæä¸‹è½½å™¨ä¹Ÿå¤±è´¥äº†...")
        print("ğŸ’¡ å»ºè®®ï¼šbioRxivå¯èƒ½éœ€è¦æ›´é«˜çº§çš„åçˆ¬è™«ç»•è¿‡æŠ€æœ¯")
        print("ğŸ’¡ æ¯”å¦‚ï¼šçœŸå®æµè§ˆå™¨æ‰©å±•ã€éªŒè¯ç è¯†åˆ«ã€æˆ–ä»˜è´¹ä»£ç†æœåŠ¡") 