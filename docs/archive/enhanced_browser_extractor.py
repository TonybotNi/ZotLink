#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæµè§ˆå™¨æå–å™¨ - åŸºäºZotero Connectorç­–ç•¥
å€Ÿé‰´å®˜æ–¹æ’ä»¶çš„ä¸‹è½½ç›‘æ§å’Œåçˆ¬è™«ç»•è¿‡æœºåˆ¶
"""

import asyncio
import logging
import os
import tempfile
from typing import Optional, Dict, Any, List
from playwright.async_api import async_playwright, Page, BrowserContext, Browser
from pathlib import Path

logger = logging.getLogger(__name__)

class EnhancedBrowserExtractor:
    """å¢å¼ºç‰ˆæµè§ˆå™¨æå–å™¨ï¼Œå®ç°ç±»ä¼¼Zotero Connectorçš„åŠŸèƒ½"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.playwright = None
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.playwright = await async_playwright().start()
        
        # ğŸš€ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨æ›´æ¥è¿‘çœŸå®æµè§ˆå™¨çš„é…ç½®
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox', 
                '--disable-dev-shm-usage',
                '--disable-web-security',
                '--allow-running-insecure-content',
                '--disable-features=VizDisplayCompositor',
                '--disable-blink-features=AutomationControlled',
                '--disable-automation-controlled',
                '--no-first-run',
                '--disable-default-apps',
                '--disable-popup-blocking',
                '--single-process',  # é¿å…è¿›ç¨‹ç®¡ç†é—®é¢˜
                '--enable-features=NetworkService,NetworkServiceLogging'
            ],
            ignore_default_args=['--enable-automation'],
            timeout=60000
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·ç¯å¢ƒ
        self.context = await self.browser.new_context(
            viewport={'width': 1366, 'height': 768},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='en-US',
            timezone_id='America/New_York',
            accept_downloads=True,  # å…è®¸ä¸‹è½½
            has_touch=False,
            is_mobile=False,
            bypass_csp=True  # ç»•è¿‡CSPé™åˆ¶
        )
        
        # è®¾ç½®å…¨å±€ä¸‹è½½è·¯å¾„
        self.download_path = tempfile.mkdtemp()
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            logger.warning(f"æ¸…ç†èµ„æºæ—¶å‡ºç°é—®é¢˜: {e}")
        
        # æ¸…ç†ä¸´æ—¶ä¸‹è½½ç›®å½•
        try:
            import shutil
            if hasattr(self, 'download_path') and os.path.exists(self.download_path):
                shutil.rmtree(self.download_path)
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸‹è½½ç›®å½•å¤±è´¥: {e}")
    
    async def extract_with_zotero_strategy(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨ç±»ä¼¼Zotero Connectorçš„ç­–ç•¥æå–å†…å®¹"""
        logger.info(f"ğŸš€ ä½¿ç”¨Zotero Connectorç­–ç•¥å¤„ç†: {url}")
        
        try:
            page = await self.context.new_page()
            
            # ğŸ¯ æ­¥éª¤1: æ™ºèƒ½é¡µé¢å¯¼èˆªå’Œç­‰å¾…
            await self._smart_page_navigation(page, url)
            
            # ğŸ¯ æ­¥éª¤2: æå–å…ƒæ•°æ®
            metadata = await self._extract_metadata_zotero_style(page, url)
            
            # ğŸ¯ æ­¥éª¤3: ä¸‹è½½PDFï¼ˆä½¿ç”¨ä¸‹è½½ç›‘æ§ï¼‰
            if metadata.get('pdf_url'):
                pdf_content = await self._download_with_monitoring(page, metadata['pdf_url'])
                if pdf_content:
                    metadata['pdf_content'] = pdf_content
            
            await page.close()
            return metadata
            
        except Exception as e:
            logger.error(f"Zoteroç­–ç•¥æå–å¤±è´¥: {e}")
            return {}
    
    async def _smart_page_navigation(self, page: Page, url: str):
        """æ™ºèƒ½é¡µé¢å¯¼èˆªï¼Œå¤„ç†åçˆ¬è™«"""
        logger.info("ğŸŒ å¼€å§‹æ™ºèƒ½é¡µé¢å¯¼èˆª...")
        
        # è®¾ç½®é¡µé¢å±æ€§
        await page.set_extra_http_headers({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # å¯¼èˆªåˆ°é¡µé¢
        timeout_ms = 45000 if 'osf.io' in url else 25000
        await page.goto(url, wait_until='domcontentloaded', timeout=timeout_ms)
        
        # æ£€æµ‹å¹¶å¤„ç†åçˆ¬è™«é¡µé¢
        await self._handle_bot_protection(page)
        
        # OSFå¹³å°ç‰¹æ®Šå¤„ç†
        if 'osf.io' in url:
            await self._handle_osf_loading(page)
    
    async def _handle_bot_protection(self, page: Page):
        """å¤„ç†åçˆ¬è™«ä¿æŠ¤ - ç±»ä¼¼Zoteroçš„å¤„ç†æ–¹å¼"""
        try:
            title = await page.title()
            if any(phrase in title.lower() for phrase in ['just a moment', 'checking', 'please wait', 'cloudflare']):
                logger.info("ğŸ›¡ï¸ æ£€æµ‹åˆ°åçˆ¬è™«é¡µé¢ï¼Œå¯ç”¨ç»•è¿‡ç­–ç•¥...")
                
                # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                await asyncio.sleep(2)
                await page.mouse.move(100, 100)
                await asyncio.sleep(1)
                
                # ç­‰å¾…åçˆ¬è™«é¡µé¢è‡ªåŠ¨è·³è½¬ï¼ˆæœ€å¤š30ç§’ï¼‰
                for i in range(15):
                    await asyncio.sleep(2)
                    new_title = await page.title()
                    if 'just a moment' not in new_title.lower():
                        logger.info(f"âœ… åçˆ¬è™«é¡µé¢å·²ç»•è¿‡: {new_title[:50]}...")
                        break
                    logger.info(f"â³ ç­‰å¾…åçˆ¬è™«æ£€æŸ¥å®Œæˆ... ({i+1}/15)")
                
                # é¢å¤–çš„äººç±»è¡Œä¸ºæ¨¡æ‹Ÿ
                await page.mouse.move(200, 200)
                await page.evaluate("window.scrollTo(0, 300)")
                await asyncio.sleep(2)
                await page.evaluate("window.scrollTo(0, 0)")
                
        except Exception as e:
            logger.info(f"åçˆ¬è™«å¤„ç†å¼‚å¸¸: {e}")
    
    async def _handle_osf_loading(self, page: Page):
        """å¤„ç†OSFå¹³å°çš„Emberåº”ç”¨åŠ è½½ - å¢å¼ºç‰ˆç­‰å¾…ç­–ç•¥"""
        logger.info("ğŸ”§ å¤„ç†OSF Emberåº”ç”¨åŠ è½½...")
        
        try:
            # ç­‰å¾…Emberåº”ç”¨å®¹å™¨
            await page.wait_for_selector('.ember-application, .ember-view', timeout=15000)
            logger.info("âœ… Emberå®¹å™¨åŠ è½½å®Œæˆ")
            
            # ğŸš€ å…³é”®æ”¹è¿›ï¼šå¾ªç¯ç­‰å¾…ç›´åˆ°æ ‡é¢˜çœŸæ­£åŠ è½½
            max_attempts = 20  # æœ€å¤šç­‰å¾…40ç§’
            for attempt in range(max_attempts):
                await asyncio.sleep(2)
                
                title = await page.title()
                logger.info(f"ç¬¬{attempt+1}æ¬¡æ£€æŸ¥ - å½“å‰æ ‡é¢˜: {title}")
                
                # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦çœŸæ­£åŠ è½½ï¼ˆåŒ…å«è®ºæ–‡ä¿¡æ¯ï¼‰
                if title and '|' in title and len(title) > 20:
                    logger.info(f"âœ… OSFå†…å®¹åŠ è½½å®Œæˆ: {title[:50]}...")
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰H1æ ‡é¢˜å‡ºç°
                h1_text = await page.evaluate("document.querySelector('h1')?.textContent?.trim() || ''")
                if h1_text and len(h1_text) > 10:
                    logger.info(f"âœ… å‘ç°H1æ ‡é¢˜: {h1_text[:50]}...")
                    break
                
                # æ¯5æ¬¡å°è¯•åè§¦å‘é¡µé¢äº¤äº’
                if (attempt + 1) % 5 == 0:
                    logger.info("ğŸ”„ è§¦å‘é¡µé¢äº¤äº’ä»¥ä¿ƒè¿›åŠ è½½...")
                    await page.evaluate("window.scrollTo(0, 300)")
                    await asyncio.sleep(1)
                    await page.evaluate("window.scrollTo(0, 0)")
                    
                    # å°è¯•ç‚¹å‡»å¯èƒ½çš„åŠ è½½æŒ‰é’®æˆ–é“¾æ¥
                    try:
                        # æ£€æŸ¥æ˜¯å¦æœ‰"show more"æˆ–ç±»ä¼¼çš„æŒ‰é’®
                        await page.evaluate("""
                            const buttons = document.querySelectorAll('button, a');
                            for (const btn of buttons) {
                                const text = btn.textContent.toLowerCase();
                                if (text.includes('load') || text.includes('show') || text.includes('expand')) {
                                    btn.click();
                                    break;
                                }
                            }
                        """)
                    except:
                        pass
                
                if attempt == max_attempts - 1:
                    logger.warning("âš ï¸ OSFå†…å®¹ç­‰å¾…è¶…æ—¶ï¼Œä½¿ç”¨å½“å‰çŠ¶æ€")
            
        except Exception as e:
            logger.info(f"OSFåŠ è½½å¤„ç†å¼‚å¸¸: {e}")
    
    async def _extract_metadata_zotero_style(self, page: Page, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨ç±»ä¼¼Zoteroçš„æ–¹å¼æå–å…ƒæ•°æ®"""
        logger.info("ğŸ“Š ä½¿ç”¨Zoteroé£æ ¼æå–å…ƒæ•°æ®...")
        
        metadata = {}
        
        # æ‰§è¡ŒJavaScriptæå–
        result = await page.evaluate("""
            () => {
                const data = {
                    title: '',
                    authors: '',
                    abstract: '',
                    DOI: '',
                    pdf_url: '',
                    date: '',
                    publicationTitle: ''
                };
                
                // ä¼˜å…ˆä»é¡µé¢æ ‡é¢˜æå–
                if (document.title) {
                    data.title = document.title;
                    
                    // OSFæ ¼å¼ç‰¹æ®Šå¤„ç†
                    if (document.title.includes('|') && window.location.hostname === 'osf.io') {
                        data.title = document.title.split('|')[1].trim();
                    }
                }
                
                // ä»H1æå–æ ‡é¢˜ï¼ˆå¤‡ç”¨ï¼‰
                if (!data.title || data.title === 'OSF') {
                    const h1 = document.querySelector('h1');
                    if (h1) data.title = h1.textContent.trim();
                }
                
                // Citation metaæ ‡ç­¾æå–
                const citationMeta = {
                    'citation_title': 'title',
                    'citation_author': 'authors',
                    'citation_publication_date': 'date',
                    'citation_doi': 'DOI',
                    'citation_pdf_url': 'pdf_url',
                    'citation_abstract_html_url': 'url'
                };
                
                for (const [metaName, field] of Object.entries(citationMeta)) {
                    const elements = document.querySelectorAll(`meta[name="${metaName}"]`);
                    if (elements.length > 0) {
                        if (metaName === 'citation_author') {
                            data.authors = Array.from(elements).map(el => el.content).join('; ');
                        } else {
                            data[field] = elements[0].content || data[field];
                        }
                    }
                }
                
                // OSFç‰¹æ®Šå¤„ç†ï¼šPDFé“¾æ¥æ„é€ 
                if (window.location.hostname === 'osf.io' && !data.pdf_url) {
                    const pathMatch = window.location.pathname.match(/\/preprints\/\\w+\/([^\\/]+)/);
                    if (pathMatch) {
                        const preprintId = pathMatch[1].replace(/_v\\d+$/, '');
                        data.pdf_url = `https://osf.io/${preprintId}/download/`;
                    }
                }
                
                // æŸ¥æ‰¾ä½œè€…é“¾æ¥ï¼ˆOSFï¼‰
                if (!data.authors && window.location.hostname === 'osf.io') {
                    const authorLinks = document.querySelectorAll('a[href*="/profile/"]');
                    if (authorLinks.length > 0) {
                        data.authors = Array.from(authorLinks)
                            .map(link => link.textContent.trim())
                            .filter(name => name)
                            .join('; ');
                    }
                }
                
                return data;
            }
        """)
        
        metadata.update(result)
        
        # ç¡®å®šæºå’Œç±»å‹
        if 'osf.io' in url:
            if 'psyarxiv' in url:
                metadata['source'] = 'PsyArXiv'
            elif 'socarxiv' in url:
                metadata['source'] = 'SocArXiv' 
            else:
                metadata['source'] = 'OSF'
            metadata['itemType'] = 'preprint'
        
        logger.info(f"ğŸ“Š å…ƒæ•°æ®æå–å®Œæˆ: æ ‡é¢˜={metadata.get('title', 'N/A')[:30]}, PDF={bool(metadata.get('pdf_url'))}")
        return metadata
    
    async def _download_with_monitoring(self, page: Page, pdf_url: str) -> Optional[bytes]:
        """ä½¿ç”¨ä¸‹è½½ç›‘æ§è·å–PDF - ç±»ä¼¼Zotero Connector"""
        logger.info(f"ğŸ“ å¯åŠ¨ä¸‹è½½ç›‘æ§: {pdf_url}")
        
        try:
            # ğŸš€ åˆ›å»ºä¸“é—¨çš„ä¸‹è½½é¡µé¢
            download_page = await self.context.new_page()
            download_content = None
            download_event = asyncio.Event()
            download_path = None
            
            def handle_download(download):
                nonlocal download_content, download_path
                logger.info(f"âœ… æ•è·ä¸‹è½½äº‹ä»¶: {download.url}")
                
                async def save_download():
                    try:
                        # å»ºè®®çš„æ–‡ä»¶å
                        import time
                        suggested_filename = download.suggested_filename or f"download_{int(time.time())}.pdf"
                        download_path = os.path.join(self.download_path, suggested_filename)
                        
                        # ä¿å­˜ä¸‹è½½æ–‡ä»¶
                        await download.save_as(download_path)
                        
                        # è¯»å–å†…å®¹
                        if os.path.exists(download_path):
                            with open(download_path, 'rb') as f:
                                content = f.read()
                            
                            if content and len(content) > 5000:
                                download_content = content
                                logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {len(content)} bytes")
                            else:
                                logger.warning(f"ä¸‹è½½æ–‡ä»¶å¤ªå°: {len(content) if content else 0} bytes")
                        
                        download_event.set()
                        
                    except Exception as e:
                        logger.error(f"ä¿å­˜ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
                        download_event.set()
                
                asyncio.create_task(save_download())
            
            # ç›‘å¬ä¸‹è½½äº‹ä»¶
            download_page.on('download', handle_download)
            
            try:
                # è®¾ç½®ä¸‹è½½é¡µé¢å±æ€§
                await download_page.set_extra_http_headers({
                    'Accept': 'application/pdf,*/*;q=0.8',
                    'Referer': page.url,
                    'Cache-Control': 'no-cache'
                })
                
                # å¯¼èˆªåˆ°PDFé“¾æ¥ï¼Œå¤„ç†"Download is starting"æƒ…å†µ
                try:
                    await download_page.goto(pdf_url, timeout=30000)
                except Exception as e:
                    # "Download is starting"å®é™…ä¸Šæ„å‘³ç€ä¸‹è½½è¢«è§¦å‘äº†
                    if "download is starting" in str(e).lower():
                        logger.info("âœ… æ£€æµ‹åˆ°ä¸‹è½½å¼€å§‹ä¿¡å·ï¼Œç­‰å¾…ä¸‹è½½å®Œæˆ...")
                    else:
                        logger.warning(f"é¡µé¢å¯¼èˆªå¼‚å¸¸: {e}")
                
                # ç­‰å¾…ä¸‹è½½äº‹ä»¶
                try:
                    await asyncio.wait_for(download_event.wait(), timeout=25.0)
                    
                    if download_content:
                        if download_content.startswith(b'%PDF'):
                            logger.info(f"ğŸ‰ PDFä¸‹è½½ç›‘æ§æˆåŠŸ: {len(download_content)} bytes")
                            return download_content
                        else:
                            logger.warning("ä¸‹è½½å†…å®¹ä¸æ˜¯PDFæ ¼å¼")
                    else:
                        logger.warning("æœªè·å–åˆ°ä¸‹è½½å†…å®¹")
                        
                except asyncio.TimeoutError:
                    logger.warning("ä¸‹è½½ç›‘æ§è¶…æ—¶")
                
            finally:
                await download_page.close()
                # æ¸…ç†ä¸‹è½½æ–‡ä»¶
                if download_path and os.path.exists(download_path):
                    try:
                        os.remove(download_path)
                    except:
                        pass
                        
        except Exception as e:
            logger.error(f"ä¸‹è½½ç›‘æ§å¼‚å¸¸: {e}")
        
        # å¤‡ç”¨ç­–ç•¥ï¼šç›´æ¥è¯·æ±‚ä¸‹è½½
        logger.info("ğŸ“ å°è¯•å¤‡ç”¨ä¸‹è½½ç­–ç•¥...")
        try:
            response = await page.context.request.get(
                pdf_url,
                headers={
                    'Accept': 'application/pdf,*/*;q=0.8',
                    'Referer': page.url
                },
                timeout=25000
            )
            
            if response.status == 200:
                 content = await response.body()
                 if content:
                     # ğŸš€ æ”¹è¿›ï¼šæ£€æŸ¥å„ç§PDFæ ¼å¼çš„å¯èƒ½æ€§
                     if content.startswith(b'%PDF'):
                         logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½æˆåŠŸ (æ ‡å‡†PDF): {len(content)} bytes")
                         return content
                     elif len(content) > 100000:  # å¤§äº100KBçš„æ–‡ä»¶
                         # å¯¹äºOSFçš„application/octet-streamï¼Œå¯èƒ½ä»ç„¶æ˜¯PDF
                         # å°è¯•æŸ¥æ‰¾PDFæ ‡è¯†ç¬¦
                         if b'%PDF' in content[:2048]:  # åœ¨å‰2KBä¸­æŸ¥æ‰¾
                             logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½æˆåŠŸ (åµŒå…¥å¼PDF): {len(content)} bytes")
                             return content
                         # æˆ–è€…æ£€æŸ¥æ˜¯å¦åŒ…å«PDFç›¸å…³å†…å®¹
                         elif (b'Adobe' in content or b'PDF' in content or 
                               response.headers.get('content-type', '').startswith('application')):
                             logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½æˆåŠŸ (å¯èƒ½æ˜¯PDF): {len(content)} bytes")
                             return content
                         else:
                             logger.warning(f"å¤‡ç”¨ä¸‹è½½è·å¾—å¤§æ–‡ä»¶ä½†æ ¼å¼æœªçŸ¥: {len(content)} bytes")
                     else:
                         logger.warning(f"å¤‡ç”¨ä¸‹è½½æ–‡ä»¶å¤ªå°: {len(content)} bytes")
                    
        except Exception as e:
            logger.info(f"å¤‡ç”¨ä¸‹è½½å¤±è´¥: {e}")
        
        logger.warning("æ‰€æœ‰ä¸‹è½½ç­–ç•¥éƒ½å¤±è´¥äº†")
        return None

# ä½¿ç”¨ç¤ºä¾‹
async def test_enhanced_extractor():
    """æµ‹è¯•å¢å¼ºç‰ˆæå–å™¨"""
    test_url = 'https://osf.io/preprints/psyarxiv/prd9y_v1'
    
    async with EnhancedBrowserExtractor() as extractor:
        result = await extractor.extract_with_zotero_strategy(test_url)
        
        print("ğŸ¯ å¢å¼ºç‰ˆæå–å™¨æµ‹è¯•ç»“æœ:")
        print(f"ğŸ“„ æ ‡é¢˜: {result.get('title', 'æœªè·å–')}")
        print(f"ğŸ‘¥ ä½œè€…: {result.get('authors', 'æœªè·å–')}")
        print(f"ğŸ”— PDFé“¾æ¥: {result.get('pdf_url', 'æœªè·å–')}")
        
        if result.get('pdf_content'):
            size_mb = len(result['pdf_content']) / (1024 * 1024)
            print(f"ğŸ“ PDFä¸‹è½½: æˆåŠŸ ({size_mb:.2f} MB)")
        else:
            print(f"ğŸ“ PDFä¸‹è½½: å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(test_enhanced_extractor()) 
"""
å¢å¼ºç‰ˆæµè§ˆå™¨æå–å™¨ - åŸºäºZotero Connectorç­–ç•¥
å€Ÿé‰´å®˜æ–¹æ’ä»¶çš„ä¸‹è½½ç›‘æ§å’Œåçˆ¬è™«ç»•è¿‡æœºåˆ¶
"""

import asyncio
import logging
import os
import tempfile
from typing import Optional, Dict, Any, List
from playwright.async_api import async_playwright, Page, BrowserContext, Browser
from pathlib import Path

logger = logging.getLogger(__name__)

class EnhancedBrowserExtractor:
    """å¢å¼ºç‰ˆæµè§ˆå™¨æå–å™¨ï¼Œå®ç°ç±»ä¼¼Zotero Connectorçš„åŠŸèƒ½"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.playwright = None
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.playwright = await async_playwright().start()
        
        # ğŸš€ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨æ›´æ¥è¿‘çœŸå®æµè§ˆå™¨çš„é…ç½®
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox', 
                '--disable-dev-shm-usage',
                '--disable-web-security',
                '--allow-running-insecure-content',
                '--disable-features=VizDisplayCompositor',
                '--disable-blink-features=AutomationControlled',
                '--disable-automation-controlled',
                '--no-first-run',
                '--disable-default-apps',
                '--disable-popup-blocking',
                '--single-process',  # é¿å…è¿›ç¨‹ç®¡ç†é—®é¢˜
                '--enable-features=NetworkService,NetworkServiceLogging'
            ],
            ignore_default_args=['--enable-automation'],
            timeout=60000
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·ç¯å¢ƒ
        self.context = await self.browser.new_context(
            viewport={'width': 1366, 'height': 768},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='en-US',
            timezone_id='America/New_York',
            accept_downloads=True,  # å…è®¸ä¸‹è½½
            has_touch=False,
            is_mobile=False,
            bypass_csp=True  # ç»•è¿‡CSPé™åˆ¶
        )
        
        # è®¾ç½®å…¨å±€ä¸‹è½½è·¯å¾„
        self.download_path = tempfile.mkdtemp()
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            logger.warning(f"æ¸…ç†èµ„æºæ—¶å‡ºç°é—®é¢˜: {e}")
        
        # æ¸…ç†ä¸´æ—¶ä¸‹è½½ç›®å½•
        try:
            import shutil
            if hasattr(self, 'download_path') and os.path.exists(self.download_path):
                shutil.rmtree(self.download_path)
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸‹è½½ç›®å½•å¤±è´¥: {e}")
    
    async def extract_with_zotero_strategy(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨ç±»ä¼¼Zotero Connectorçš„ç­–ç•¥æå–å†…å®¹"""
        logger.info(f"ğŸš€ ä½¿ç”¨Zotero Connectorç­–ç•¥å¤„ç†: {url}")
        
        try:
            page = await self.context.new_page()
            
            # ğŸ¯ æ­¥éª¤1: æ™ºèƒ½é¡µé¢å¯¼èˆªå’Œç­‰å¾…
            await self._smart_page_navigation(page, url)
            
            # ğŸ¯ æ­¥éª¤2: æå–å…ƒæ•°æ®
            metadata = await self._extract_metadata_zotero_style(page, url)
            
            # ğŸ¯ æ­¥éª¤3: ä¸‹è½½PDFï¼ˆä½¿ç”¨ä¸‹è½½ç›‘æ§ï¼‰
            if metadata.get('pdf_url'):
                pdf_content = await self._download_with_monitoring(page, metadata['pdf_url'])
                if pdf_content:
                    metadata['pdf_content'] = pdf_content
            
            await page.close()
            return metadata
            
        except Exception as e:
            logger.error(f"Zoteroç­–ç•¥æå–å¤±è´¥: {e}")
            return {}
    
    async def _smart_page_navigation(self, page: Page, url: str):
        """æ™ºèƒ½é¡µé¢å¯¼èˆªï¼Œå¤„ç†åçˆ¬è™«"""
        logger.info("ğŸŒ å¼€å§‹æ™ºèƒ½é¡µé¢å¯¼èˆª...")
        
        # è®¾ç½®é¡µé¢å±æ€§
        await page.set_extra_http_headers({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # å¯¼èˆªåˆ°é¡µé¢
        timeout_ms = 45000 if 'osf.io' in url else 25000
        await page.goto(url, wait_until='domcontentloaded', timeout=timeout_ms)
        
        # æ£€æµ‹å¹¶å¤„ç†åçˆ¬è™«é¡µé¢
        await self._handle_bot_protection(page)
        
        # OSFå¹³å°ç‰¹æ®Šå¤„ç†
        if 'osf.io' in url:
            await self._handle_osf_loading(page)
    
    async def _handle_bot_protection(self, page: Page):
        """å¤„ç†åçˆ¬è™«ä¿æŠ¤ - ç±»ä¼¼Zoteroçš„å¤„ç†æ–¹å¼"""
        try:
            title = await page.title()
            if any(phrase in title.lower() for phrase in ['just a moment', 'checking', 'please wait', 'cloudflare']):
                logger.info("ğŸ›¡ï¸ æ£€æµ‹åˆ°åçˆ¬è™«é¡µé¢ï¼Œå¯ç”¨ç»•è¿‡ç­–ç•¥...")
                
                # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                await asyncio.sleep(2)
                await page.mouse.move(100, 100)
                await asyncio.sleep(1)
                
                # ç­‰å¾…åçˆ¬è™«é¡µé¢è‡ªåŠ¨è·³è½¬ï¼ˆæœ€å¤š30ç§’ï¼‰
                for i in range(15):
                    await asyncio.sleep(2)
                    new_title = await page.title()
                    if 'just a moment' not in new_title.lower():
                        logger.info(f"âœ… åçˆ¬è™«é¡µé¢å·²ç»•è¿‡: {new_title[:50]}...")
                        break
                    logger.info(f"â³ ç­‰å¾…åçˆ¬è™«æ£€æŸ¥å®Œæˆ... ({i+1}/15)")
                
                # é¢å¤–çš„äººç±»è¡Œä¸ºæ¨¡æ‹Ÿ
                await page.mouse.move(200, 200)
                await page.evaluate("window.scrollTo(0, 300)")
                await asyncio.sleep(2)
                await page.evaluate("window.scrollTo(0, 0)")
                
        except Exception as e:
            logger.info(f"åçˆ¬è™«å¤„ç†å¼‚å¸¸: {e}")
    
    async def _handle_osf_loading(self, page: Page):
        """å¤„ç†OSFå¹³å°çš„Emberåº”ç”¨åŠ è½½ - å¢å¼ºç‰ˆç­‰å¾…ç­–ç•¥"""
        logger.info("ğŸ”§ å¤„ç†OSF Emberåº”ç”¨åŠ è½½...")
        
        try:
            # ç­‰å¾…Emberåº”ç”¨å®¹å™¨
            await page.wait_for_selector('.ember-application, .ember-view', timeout=15000)
            logger.info("âœ… Emberå®¹å™¨åŠ è½½å®Œæˆ")
            
            # ğŸš€ å…³é”®æ”¹è¿›ï¼šå¾ªç¯ç­‰å¾…ç›´åˆ°æ ‡é¢˜çœŸæ­£åŠ è½½
            max_attempts = 20  # æœ€å¤šç­‰å¾…40ç§’
            for attempt in range(max_attempts):
                await asyncio.sleep(2)
                
                title = await page.title()
                logger.info(f"ç¬¬{attempt+1}æ¬¡æ£€æŸ¥ - å½“å‰æ ‡é¢˜: {title}")
                
                # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦çœŸæ­£åŠ è½½ï¼ˆåŒ…å«è®ºæ–‡ä¿¡æ¯ï¼‰
                if title and '|' in title and len(title) > 20:
                    logger.info(f"âœ… OSFå†…å®¹åŠ è½½å®Œæˆ: {title[:50]}...")
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰H1æ ‡é¢˜å‡ºç°
                h1_text = await page.evaluate("document.querySelector('h1')?.textContent?.trim() || ''")
                if h1_text and len(h1_text) > 10:
                    logger.info(f"âœ… å‘ç°H1æ ‡é¢˜: {h1_text[:50]}...")
                    break
                
                # æ¯5æ¬¡å°è¯•åè§¦å‘é¡µé¢äº¤äº’
                if (attempt + 1) % 5 == 0:
                    logger.info("ğŸ”„ è§¦å‘é¡µé¢äº¤äº’ä»¥ä¿ƒè¿›åŠ è½½...")
                    await page.evaluate("window.scrollTo(0, 300)")
                    await asyncio.sleep(1)
                    await page.evaluate("window.scrollTo(0, 0)")
                    
                    # å°è¯•ç‚¹å‡»å¯èƒ½çš„åŠ è½½æŒ‰é’®æˆ–é“¾æ¥
                    try:
                        # æ£€æŸ¥æ˜¯å¦æœ‰"show more"æˆ–ç±»ä¼¼çš„æŒ‰é’®
                        await page.evaluate("""
                            const buttons = document.querySelectorAll('button, a');
                            for (const btn of buttons) {
                                const text = btn.textContent.toLowerCase();
                                if (text.includes('load') || text.includes('show') || text.includes('expand')) {
                                    btn.click();
                                    break;
                                }
                            }
                        """)
                    except:
                        pass
                
                if attempt == max_attempts - 1:
                    logger.warning("âš ï¸ OSFå†…å®¹ç­‰å¾…è¶…æ—¶ï¼Œä½¿ç”¨å½“å‰çŠ¶æ€")
            
        except Exception as e:
            logger.info(f"OSFåŠ è½½å¤„ç†å¼‚å¸¸: {e}")
    
    async def _extract_metadata_zotero_style(self, page: Page, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨ç±»ä¼¼Zoteroçš„æ–¹å¼æå–å…ƒæ•°æ®"""
        logger.info("ğŸ“Š ä½¿ç”¨Zoteroé£æ ¼æå–å…ƒæ•°æ®...")
        
        metadata = {}
        
        # æ‰§è¡ŒJavaScriptæå–
        result = await page.evaluate("""
            () => {
                const data = {
                    title: '',
                    authors: '',
                    abstract: '',
                    DOI: '',
                    pdf_url: '',
                    date: '',
                    publicationTitle: ''
                };
                
                // ä¼˜å…ˆä»é¡µé¢æ ‡é¢˜æå–
                if (document.title) {
                    data.title = document.title;
                    
                    // OSFæ ¼å¼ç‰¹æ®Šå¤„ç†
                    if (document.title.includes('|') && window.location.hostname === 'osf.io') {
                        data.title = document.title.split('|')[1].trim();
                    }
                }
                
                // ä»H1æå–æ ‡é¢˜ï¼ˆå¤‡ç”¨ï¼‰
                if (!data.title || data.title === 'OSF') {
                    const h1 = document.querySelector('h1');
                    if (h1) data.title = h1.textContent.trim();
                }
                
                // Citation metaæ ‡ç­¾æå–
                const citationMeta = {
                    'citation_title': 'title',
                    'citation_author': 'authors',
                    'citation_publication_date': 'date',
                    'citation_doi': 'DOI',
                    'citation_pdf_url': 'pdf_url',
                    'citation_abstract_html_url': 'url'
                };
                
                for (const [metaName, field] of Object.entries(citationMeta)) {
                    const elements = document.querySelectorAll(`meta[name="${metaName}"]`);
                    if (elements.length > 0) {
                        if (metaName === 'citation_author') {
                            data.authors = Array.from(elements).map(el => el.content).join('; ');
                        } else {
                            data[field] = elements[0].content || data[field];
                        }
                    }
                }
                
                // OSFç‰¹æ®Šå¤„ç†ï¼šPDFé“¾æ¥æ„é€ 
                if (window.location.hostname === 'osf.io' && !data.pdf_url) {
                    const pathMatch = window.location.pathname.match(/\/preprints\/\\w+\/([^\\/]+)/);
                    if (pathMatch) {
                        const preprintId = pathMatch[1].replace(/_v\\d+$/, '');
                        data.pdf_url = `https://osf.io/${preprintId}/download/`;
                    }
                }
                
                // æŸ¥æ‰¾ä½œè€…é“¾æ¥ï¼ˆOSFï¼‰
                if (!data.authors && window.location.hostname === 'osf.io') {
                    const authorLinks = document.querySelectorAll('a[href*="/profile/"]');
                    if (authorLinks.length > 0) {
                        data.authors = Array.from(authorLinks)
                            .map(link => link.textContent.trim())
                            .filter(name => name)
                            .join('; ');
                    }
                }
                
                return data;
            }
        """)
        
        metadata.update(result)
        
        # ç¡®å®šæºå’Œç±»å‹
        if 'osf.io' in url:
            if 'psyarxiv' in url:
                metadata['source'] = 'PsyArXiv'
            elif 'socarxiv' in url:
                metadata['source'] = 'SocArXiv' 
            else:
                metadata['source'] = 'OSF'
            metadata['itemType'] = 'preprint'
        
        logger.info(f"ğŸ“Š å…ƒæ•°æ®æå–å®Œæˆ: æ ‡é¢˜={metadata.get('title', 'N/A')[:30]}, PDF={bool(metadata.get('pdf_url'))}")
        return metadata
    
    async def _download_with_monitoring(self, page: Page, pdf_url: str) -> Optional[bytes]:
        """ä½¿ç”¨ä¸‹è½½ç›‘æ§è·å–PDF - ç±»ä¼¼Zotero Connector"""
        logger.info(f"ğŸ“ å¯åŠ¨ä¸‹è½½ç›‘æ§: {pdf_url}")
        
        try:
            # ğŸš€ åˆ›å»ºä¸“é—¨çš„ä¸‹è½½é¡µé¢
            download_page = await self.context.new_page()
            download_content = None
            download_event = asyncio.Event()
            download_path = None
            
            def handle_download(download):
                nonlocal download_content, download_path
                logger.info(f"âœ… æ•è·ä¸‹è½½äº‹ä»¶: {download.url}")
                
                async def save_download():
                    try:
                        # å»ºè®®çš„æ–‡ä»¶å
                        import time
                        suggested_filename = download.suggested_filename or f"download_{int(time.time())}.pdf"
                        download_path = os.path.join(self.download_path, suggested_filename)
                        
                        # ä¿å­˜ä¸‹è½½æ–‡ä»¶
                        await download.save_as(download_path)
                        
                        # è¯»å–å†…å®¹
                        if os.path.exists(download_path):
                            with open(download_path, 'rb') as f:
                                content = f.read()
                            
                            if content and len(content) > 5000:
                                download_content = content
                                logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {len(content)} bytes")
                            else:
                                logger.warning(f"ä¸‹è½½æ–‡ä»¶å¤ªå°: {len(content) if content else 0} bytes")
                        
                        download_event.set()
                        
                    except Exception as e:
                        logger.error(f"ä¿å­˜ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
                        download_event.set()
                
                asyncio.create_task(save_download())
            
            # ç›‘å¬ä¸‹è½½äº‹ä»¶
            download_page.on('download', handle_download)
            
            try:
                # è®¾ç½®ä¸‹è½½é¡µé¢å±æ€§
                await download_page.set_extra_http_headers({
                    'Accept': 'application/pdf,*/*;q=0.8',
                    'Referer': page.url,
                    'Cache-Control': 'no-cache'
                })
                
                # å¯¼èˆªåˆ°PDFé“¾æ¥ï¼Œå¤„ç†"Download is starting"æƒ…å†µ
                try:
                    await download_page.goto(pdf_url, timeout=30000)
                except Exception as e:
                    # "Download is starting"å®é™…ä¸Šæ„å‘³ç€ä¸‹è½½è¢«è§¦å‘äº†
                    if "download is starting" in str(e).lower():
                        logger.info("âœ… æ£€æµ‹åˆ°ä¸‹è½½å¼€å§‹ä¿¡å·ï¼Œç­‰å¾…ä¸‹è½½å®Œæˆ...")
                    else:
                        logger.warning(f"é¡µé¢å¯¼èˆªå¼‚å¸¸: {e}")
                
                # ç­‰å¾…ä¸‹è½½äº‹ä»¶
                try:
                    await asyncio.wait_for(download_event.wait(), timeout=25.0)
                    
                    if download_content:
                        if download_content.startswith(b'%PDF'):
                            logger.info(f"ğŸ‰ PDFä¸‹è½½ç›‘æ§æˆåŠŸ: {len(download_content)} bytes")
                            return download_content
                        else:
                            logger.warning("ä¸‹è½½å†…å®¹ä¸æ˜¯PDFæ ¼å¼")
                    else:
                        logger.warning("æœªè·å–åˆ°ä¸‹è½½å†…å®¹")
                        
                except asyncio.TimeoutError:
                    logger.warning("ä¸‹è½½ç›‘æ§è¶…æ—¶")
                
            finally:
                await download_page.close()
                # æ¸…ç†ä¸‹è½½æ–‡ä»¶
                if download_path and os.path.exists(download_path):
                    try:
                        os.remove(download_path)
                    except:
                        pass
                        
        except Exception as e:
            logger.error(f"ä¸‹è½½ç›‘æ§å¼‚å¸¸: {e}")
        
        # å¤‡ç”¨ç­–ç•¥ï¼šç›´æ¥è¯·æ±‚ä¸‹è½½
        logger.info("ğŸ“ å°è¯•å¤‡ç”¨ä¸‹è½½ç­–ç•¥...")
        try:
            response = await page.context.request.get(
                pdf_url,
                headers={
                    'Accept': 'application/pdf,*/*;q=0.8',
                    'Referer': page.url
                },
                timeout=25000
            )
            
            if response.status == 200:
                 content = await response.body()
                 if content:
                     # ğŸš€ æ”¹è¿›ï¼šæ£€æŸ¥å„ç§PDFæ ¼å¼çš„å¯èƒ½æ€§
                     if content.startswith(b'%PDF'):
                         logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½æˆåŠŸ (æ ‡å‡†PDF): {len(content)} bytes")
                         return content
                     elif len(content) > 100000:  # å¤§äº100KBçš„æ–‡ä»¶
                         # å¯¹äºOSFçš„application/octet-streamï¼Œå¯èƒ½ä»ç„¶æ˜¯PDF
                         # å°è¯•æŸ¥æ‰¾PDFæ ‡è¯†ç¬¦
                         if b'%PDF' in content[:2048]:  # åœ¨å‰2KBä¸­æŸ¥æ‰¾
                             logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½æˆåŠŸ (åµŒå…¥å¼PDF): {len(content)} bytes")
                             return content
                         # æˆ–è€…æ£€æŸ¥æ˜¯å¦åŒ…å«PDFç›¸å…³å†…å®¹
                         elif (b'Adobe' in content or b'PDF' in content or 
                               response.headers.get('content-type', '').startswith('application')):
                             logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½æˆåŠŸ (å¯èƒ½æ˜¯PDF): {len(content)} bytes")
                             return content
                         else:
                             logger.warning(f"å¤‡ç”¨ä¸‹è½½è·å¾—å¤§æ–‡ä»¶ä½†æ ¼å¼æœªçŸ¥: {len(content)} bytes")
                     else:
                         logger.warning(f"å¤‡ç”¨ä¸‹è½½æ–‡ä»¶å¤ªå°: {len(content)} bytes")
                    
        except Exception as e:
            logger.info(f"å¤‡ç”¨ä¸‹è½½å¤±è´¥: {e}")
        
        logger.warning("æ‰€æœ‰ä¸‹è½½ç­–ç•¥éƒ½å¤±è´¥äº†")
        return None

# ä½¿ç”¨ç¤ºä¾‹
async def test_enhanced_extractor():
    """æµ‹è¯•å¢å¼ºç‰ˆæå–å™¨"""
    test_url = 'https://osf.io/preprints/psyarxiv/prd9y_v1'
    
    async with EnhancedBrowserExtractor() as extractor:
        result = await extractor.extract_with_zotero_strategy(test_url)
        
        print("ğŸ¯ å¢å¼ºç‰ˆæå–å™¨æµ‹è¯•ç»“æœ:")
        print(f"ğŸ“„ æ ‡é¢˜: {result.get('title', 'æœªè·å–')}")
        print(f"ğŸ‘¥ ä½œè€…: {result.get('authors', 'æœªè·å–')}")
        print(f"ğŸ”— PDFé“¾æ¥: {result.get('pdf_url', 'æœªè·å–')}")
        
        if result.get('pdf_content'):
            size_mb = len(result['pdf_content']) / (1024 * 1024)
            print(f"ğŸ“ PDFä¸‹è½½: æˆåŠŸ ({size_mb:.2f} MB)")
        else:
            print(f"ğŸ“ PDFä¸‹è½½: å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(test_enhanced_extractor()) 