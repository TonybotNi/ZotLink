#!/usr/bin/env python3
"""
ğŸ›¡ï¸ åçˆ¬è™«PDFä¸‹è½½å™¨ - å¢å¼ºç‰ˆ
ä¸“é—¨ç”¨äºä¸‹è½½bioRxivç­‰åçˆ¬è™«ç½‘ç«™çš„PDFå†…å®¹
"""

import asyncio
import logging
import tempfile
import os
import time
from typing import Optional
from pathlib import Path

logger = logging.getLogger(__name__)

async def download_anti_crawler_pdf_async(pdf_url: str) -> Optional[bytes]:
    """
    ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼ä¸‹è½½åçˆ¬è™«ç½‘ç«™çš„PDF - å¢å¼ºç‰ˆ
    
    Args:
        pdf_url: PDFé“¾æ¥
        
    Returns:
        PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        from playwright.async_api import async_playwright
        
        logger.info(f"ğŸŒ å¯åŠ¨æµè§ˆå™¨ä¸‹è½½PDF: {pdf_url}")
        
        playwright = await async_playwright().start()
        
        # ä½¿ç”¨æ›´çœŸå®çš„æµè§ˆå™¨é…ç½®
        browser = await playwright.chromium.launch(
            headless=False,  # ä½¿ç”¨æœ‰å¤´æ¨¡å¼æ›´å®¹æ˜“ç»•è¿‡æ£€æµ‹
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-web-security',
                '--disable-features=IsolateOrigins,site-per-process'
            ]
        )
        
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='en-US',
            timezone_id='America/New_York',
            permissions=['geolocation', 'notifications'],
            accept_downloads=True,  # æ˜ç¡®æ¥å—ä¸‹è½½
            ignore_https_errors=True
        )
        
        # æ·»åŠ é¢å¤–çš„æµè§ˆå™¨æ ‡è¯†æ¥ç»•è¿‡æ£€æµ‹
        await context.add_init_script("""
            // ç§»é™¤webdriveræ ‡è¯†
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            // æ·»åŠ Chromeç‰¹æ€§
            window.chrome = {
                runtime: {},
                loadTimes: function() {},
                csi: function() {},
                app: {}
            };
            
            // ä¿®æ”¹navigator.plugins
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            // ä¿®æ”¹navigator.languages
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en']
            });
        """)
        
        page = await context.new_page()
        
        # è®¾ç½®é¢å¤–çš„headers
        await page.set_extra_http_headers({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })
        
        try:
            # æ–¹æ³•1ï¼šç›‘å¬ä¸‹è½½äº‹ä»¶
            download_info = {'completed': False, 'content': None, 'error': None}
            
            async def handle_download(download):
                nonlocal download_info
                try:
                    logger.info(f"ğŸ¯ æ£€æµ‹åˆ°PDFä¸‹è½½: {download.url}")
                    
                    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜ä¸‹è½½å†…å®¹
                    temp_dir = tempfile.gettempdir()
                    temp_path = os.path.join(temp_dir, f"temp_pdf_{int(time.time())}.pdf")
                    
                    await download.save_as(temp_path)
                    
                    # è¯»å–ä¸‹è½½çš„æ–‡ä»¶å†…å®¹
                    with open(temp_path, 'rb') as f:
                        pdf_bytes = f.read()
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
                    
                    if pdf_bytes and pdf_bytes.startswith(b'%PDF'):
                        download_info['content'] = pdf_bytes
                        download_info['completed'] = True
                        logger.info(f"âœ… æµè§ˆå™¨ä¸‹è½½æˆåŠŸ: {len(pdf_bytes)} bytes")
                    else:
                        download_info['error'] = "ä¸‹è½½çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆPDF"
                        
                except Exception as e:
                    download_info['error'] = str(e)
                    logger.error(f"âŒ ä¸‹è½½å¤„ç†å¼‚å¸¸: {e}")
            
            # è®¾ç½®ä¸‹è½½ç›‘å¬
            page.on("download", handle_download)
            
            # æ–¹æ³•2ï¼šå¦‚æœæ˜¯bioRxivï¼Œå…ˆè®¿é—®ä¸»é¡µé¢è·å–cookie
            if 'biorxiv.org' in pdf_url or 'medrxiv.org' in pdf_url:
                # ä»PDF URLæ„å»ºä¸»é¡µé¢URL
                import re
                match = re.search(r'(\d+\.\d+/\d+\.\d+\.\d+\.\d+v\d+)', pdf_url)
                if match:
                    doi = match.group(1)
                    main_url = f"https://www.biorxiv.org/content/{doi}"
                    
                    logger.info(f"ğŸ“„ å…ˆè®¿é—®ä¸»é¡µé¢: {main_url}")
                    
                    # è®¿é—®ä¸»é¡µé¢è·å–cookieå’Œsession
                    try:
                        await page.goto(main_url, wait_until='networkidle', timeout=30000)
                        await asyncio.sleep(2)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                        
                        # æŸ¥æ‰¾å¹¶ç‚¹å‡»PDFé“¾æ¥
                        pdf_link = await page.query_selector('a[href*=".full.pdf"]')
                        if pdf_link:
                            logger.info("ğŸ–±ï¸ æ‰¾åˆ°PDFé“¾æ¥ï¼Œç‚¹å‡»ä¸‹è½½...")
                            
                            # ç‚¹å‡»é“¾æ¥è§¦å‘ä¸‹è½½
                            await pdf_link.click()
                            
                            # ç­‰å¾…ä¸‹è½½å®Œæˆ
                            for i in range(30):
                                if download_info['completed'] or download_info['error']:
                                    break
                                await asyncio.sleep(1)
                            
                            if download_info['completed']:
                                return download_info['content']
                    except Exception as e:
                        logger.warning(f"âš ï¸ ä¸»é¡µé¢æ–¹æ³•å¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šç›´æ¥å¯¼èˆªåˆ°PDF URL
            logger.info("ğŸ“¥ å°è¯•ç›´æ¥è®¿é—®PDF URL...")
            try:
                # ä½¿ç”¨responseäº‹ä»¶æ¥æ•è·PDFå†…å®¹
                pdf_content = None
                
                async def handle_response(response):
                    nonlocal pdf_content
                    if response.url == pdf_url and response.status == 200:
                        try:
                            body = await response.body()
                            if body and body.startswith(b'%PDF'):
                                pdf_content = body
                                logger.info(f"âœ… ä»å“åº”è·å–PDF: {len(body)} bytes")
                        except:
                            pass
                
                page.on("response", handle_response)
                
                await page.goto(pdf_url, wait_until='networkidle', timeout=30000)
                
                # ç­‰å¾…ä¸‹è½½æˆ–å“åº”
                for i in range(20):
                    if download_info['completed'] or pdf_content:
                        break
                    await asyncio.sleep(1)
                
                if download_info['completed']:
                    return download_info['content']
                elif pdf_content:
                    return pdf_content
                    
            except Exception as e:
                # ä¸‹è½½è¢«è§¦å‘æ—¶ä¼šæœ‰å¼‚å¸¸ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                logger.info(f"âš¡ å¯¼èˆªå¼‚å¸¸ï¼ˆå¯èƒ½æ˜¯ä¸‹è½½è§¦å‘ï¼‰: {str(e)[:100]}")
                
                # ç­‰å¾…ä¸‹è½½å®Œæˆ
                for i in range(20):
                    if download_info['completed'] or download_info['error']:
                        break
                    await asyncio.sleep(1)
                
                if download_info['completed']:
                    return download_info['content']
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›None
            if download_info['error']:
                logger.warning(f"âš ï¸ ä¸‹è½½å¤±è´¥: {download_info['error']}")
            else:
                logger.warning("âš ï¸ ä¸‹è½½è¶…æ—¶æˆ–æœªè§¦å‘")
            
            return None
            
        finally:
            await page.close()
            await context.close()
            await browser.close()
            await playwright.stop()
            
    except Exception as e:
        logger.error(f"âŒ æµè§ˆå™¨PDFä¸‹è½½å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return None


def download_anti_crawler_pdf(pdf_url: str) -> Optional[bytes]:
    """
    åŒæ­¥åŒ…è£…å™¨ï¼šä¸‹è½½åçˆ¬è™«ç½‘ç«™çš„PDF
    
    Args:
        pdf_url: PDFé“¾æ¥
        
    Returns:
        PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(download_anti_crawler_pdf_async(pdf_url))
        finally:
            loop.close()
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥ä¸‹è½½å¼‚å¸¸: {e}")
        return None


def is_anti_crawler_site(url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºåçˆ¬è™«ç½‘ç«™"""
    anti_crawler_domains = [
        'biorxiv.org', 'medrxiv.org', 'chemrxiv.org', 
        'psyarxiv.com', 'socarxiv.org', 'osf.io'
    ]
    return any(domain in url.lower() for domain in anti_crawler_domains)


if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•ä¸‹è½½
    test_urls = [
        "https://www.biorxiv.org/content/10.1101/2025.09.21.677607v1.full.pdf",
        "https://www.biorxiv.org/content/10.1101/2025.09.22.677711v1.full.pdf"
    ]
    
    for test_url in test_urls:
        print(f"\nğŸ§ª æµ‹è¯•ä¸‹è½½: {test_url}")
        
        content = download_anti_crawler_pdf(test_url)
        
        if content:
            print(f"âœ… ä¸‹è½½æˆåŠŸ: {len(content)} bytes")
            
            # ä¿å­˜æµ‹è¯•æ–‡ä»¶
            filename = f'/tmp/test_biorxiv_{int(time.time())}.pdf'
            with open(filename, 'wb') as f:
                f.write(content)
            print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶å·²ä¿å­˜åˆ° {filename}")
        else:
            print("âŒ ä¸‹è½½å¤±è´¥") 