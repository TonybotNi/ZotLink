"""
Nature Scholar Tool MCP æœåŠ¡å™¨
è½»é‡çº§ç‰ˆæœ¬ - åŸºäºrequests + cookiesï¼Œæ— éœ€Selenium
"""
import asyncio
import logging
import os
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv

from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource, 
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
    ReadResourceResult
)
from pydantic import AnyUrl

from .downloader import LightweightNatureDownloader
from .zotero_integration import ZoteroConnector

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–MCPæœåŠ¡å™¨
app = Server("nature-scholar-tool")

# å…¨å±€å˜é‡
downloader = None

# ç‹¬ç«‹çš„Zoteroè¿æ¥å™¨ï¼ˆä¸ä¾èµ–cookiesï¼‰
zotero_connector = ZoteroConnector()


@app.list_resources()
async def handle_list_resources() -> list[Resource]:
    """åˆ—å‡ºå¯ç”¨èµ„æº"""
    resources = [
        Resource(
            uri=AnyUrl("nature://config"),
            name="é…ç½®ä¿¡æ¯",
            description="Nature Scholar Toolçš„é…ç½®ä¿¡æ¯",
            mimeType="application/json"
        ),
        Resource(
            uri=AnyUrl("nature://status"),
            name="ç™»å½•çŠ¶æ€",
            description="å½“å‰ç™»å½•çŠ¶æ€å’Œä¼šè¯ä¿¡æ¯",
            mimeType="application/json"
        ),
        Resource(
            uri=AnyUrl("nature://cookies-guide"),
            name="Cookiesè·å–æŒ‡å—",
            description="å¦‚ä½•è·å–Natureç½‘ç«™cookiesçš„è¯¦ç»†æŒ‡å—",
            mimeType="text/markdown"
        ),
        Resource(
            uri=AnyUrl("nature://downloads"),
            name="ä¸‹è½½å†å²",
            description="å·²ä¸‹è½½æ–‡çŒ®çš„å†å²è®°å½•",
            mimeType="application/json"
        )
    ]
    return resources


@app.read_resource()
async def handle_read_resource(uri: AnyUrl) -> ReadResourceResult:
    """è¯»å–èµ„æºå†…å®¹"""
    try:
        if uri.scheme != "nature":
            raise ValueError(f"ä¸æ”¯æŒçš„URIåè®®: {uri.scheme}")
        
        path = uri.path
        
        if path == "config":
            config_info = {
                "tool_name": "Nature Scholar Tool",
                "version": "0.2.0",
                "description": "è½»é‡çº§Natureæ–‡çŒ®ä¸‹è½½å·¥å…· - åŸºäºrequests + cookies",
                "supported_features": [
                    "åŸºäºcookiesçš„æ— å¤´ä¸‹è½½",
                    "è®ºæ–‡æœç´¢",
                    "PDFä¸‹è½½",
                    "HTMLå†…å®¹ä¿å­˜",
                    "è¡¥å……ææ–™ä¸‹è½½",
                    "è‡ªåŠ¨ä»æµè§ˆå™¨è¯»å–cookies",
                    "æ‰‹åŠ¨cookieså¯¼å…¥"
                ],
                "download_directory": os.path.expanduser("~/Downloads/Nature_Papers"),
                "no_selenium_required": True
            }
            return ReadResourceResult(
                contents=[TextContent(type="text", text=str(config_info))]
            )
        
        elif path == "status":
            global downloader
            if downloader:
                status = downloader.test_login_status()
                status_info = {
                    "session_active": True,
                    "logged_in": status.get("logged_in", False),
                    "cookies_count": status.get("cookies_count", 0),
                    "last_test": status
                }
            else:
                status_info = {
                    "session_active": False,
                    "logged_in": False,
                    "cookies_count": 0,
                    "message": "å°šæœªåˆå§‹åŒ–ä¸‹è½½å™¨"
                }
            
            return ReadResourceResult(
                contents=[TextContent(type="text", text=str(status_info))]
            )
        
        elif path == "cookies-guide":
            if downloader:
                guide = downloader.get_cookies_from_browser_manual()
            else:
                temp_downloader = LightweightNatureDownloader()
                guide = temp_downloader.get_cookies_from_browser_manual()
            
            return ReadResourceResult(
                contents=[TextContent(type="text", text=guide)]
            )
        
        elif path == "downloads":
            download_info = {
                "total_downloads": 0,
                "recent_downloads": [],
                "download_directory": os.path.expanduser("~/Downloads/Nature_Papers"),
                "session_active": downloader is not None
            }
            return ReadResourceResult(
                contents=[TextContent(type="text", text=str(download_info))]
            )
        
        else:
            raise ValueError(f"æœªçŸ¥èµ„æºè·¯å¾„: {path}")
    
    except Exception as e:
        logger.error(f"è¯»å–èµ„æºæ—¶å‡ºé”™: {str(e)}")
        return ReadResourceResult(
            contents=[TextContent(type="text", text=f"è¯»å–èµ„æºæ—¶å‡ºé”™: {str(e)}")]
        )


@app.list_tools()
async def handle_list_tools() -> list[Tool]:
    """åˆ—å‡ºå¯ç”¨å·¥å…·"""
    tools = [
        Tool(
            name="set_cookies",
            description="è®¾ç½®Natureç½‘ç«™çš„cookiesï¼ˆJSONæ ¼å¼æˆ–cookieå­—ç¬¦ä¸²ï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "cookies": {
                        "type": "string",
                        "description": "Cookiesæ•°æ®ï¼ˆJSONæ ¼å¼æˆ–cookieå­—ç¬¦ä¸²ï¼‰"
                    }
                },
                "required": ["cookies"]
            }
        ),
        Tool(
            name="load_cookies_from_browser",
            description="ä»æµè§ˆå™¨è‡ªåŠ¨è¯»å–cookies",
            inputSchema={
                "type": "object",
                "properties": {
                    "browser": {
                        "type": "string",
                        "description": "æµè§ˆå™¨ç±»å‹ï¼ˆchrome, safariï¼‰",
                        "default": "chrome",
                        "enum": ["chrome", "safari"]
                    }
                }
            }
        ),
        Tool(
            name="test_login_status",
            description="æµ‹è¯•å½“å‰ç™»å½•çŠ¶æ€",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="search_papers",
            description="åœ¨Natureç½‘ç«™æœç´¢è®ºæ–‡",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "æœ€å¤§ç»“æœæ•°é‡ï¼ˆé»˜è®¤10ï¼‰",
                        "default": 10,
                        "minimum": 1,
                        "maximum": 50
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="enhanced_search_papers",
            description="å¢å¼ºçš„å…³é”®è¯æœç´¢ï¼Œè¿”å›æœ€ç›¸å…³çš„10ç¯‡æ–‡çŒ®ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "keywords": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "æœ€å¤§è¿”å›æ•°é‡ï¼ˆé»˜è®¤10ï¼‰",
                        "default": 10,
                        "minimum": 1,
                        "maximum": 20
                    }
                },
                "required": ["keywords"]
            }
        ),
        Tool(
            name="download_paper",
            description="ä¸‹è½½æŒ‡å®šçš„è®ºæ–‡",
            inputSchema={
                "type": "object",
                "properties": {
                    "paper_url": {
                        "type": "string",
                        "description": "è®ºæ–‡çš„URL"
                    },
                    "paper_title": {
                        "type": "string",
                        "description": "è®ºæ–‡æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºç”Ÿæˆæ–‡ä»¶åï¼‰",
                        "default": ""
                    }
                },
                "required": ["paper_url"]
            }
        ),
        Tool(
            name="download_papers_batch",
            description="æ‰¹é‡ä¸‹è½½è®ºæ–‡",
            inputSchema={
                "type": "object",
                "properties": {
                    "papers": {
                        "type": "array",
                        "description": "è®ºæ–‡ä¿¡æ¯æ•°ç»„",
                        "items": {
                            "type": "object",
                            "properties": {
                                "url": {"type": "string"},
                                "title": {"type": "string"}
                            },
                            "required": ["url"]
                        }
                    }
                },
                "required": ["papers"]
            }
        ),
        Tool(
            name="get_cookies_guide",
            description="è·å–å¦‚ä½•æ‰‹åŠ¨è·å–cookiesçš„æŒ‡å¯¼",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="export_cookies",
            description="å¯¼å‡ºå½“å‰cookiesä¸ºJSONæ ¼å¼",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="check_zotero_status",
            description="æ£€æŸ¥Zoteroè¿æ¥çŠ¶æ€",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="get_zotero_collections",
            description="è·å–Zoteroé›†åˆåˆ—è¡¨",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="save_to_zotero",
            description="å°†è®ºæ–‡ä¿¡æ¯ä¿å­˜åˆ°Zotero",
            inputSchema={
                "type": "object",
                "properties": {
                    "paper_url": {
                        "type": "string",
                        "description": "è®ºæ–‡URL"
                    },
                    "paper_title": {
                        "type": "string",
                        "description": "è®ºæ–‡æ ‡é¢˜"
                    },
                    "collection_key": {
                        "type": "string",
                        "description": "ç›®æ ‡é›†åˆkeyï¼ˆå¯é€‰ï¼‰"
                    }
                },
                "required": ["paper_url", "paper_title"]
            }
        ),
        Tool(
            name="create_zotero_collection",
            description="åœ¨Zoteroä¸­åˆ›å»ºæ–°é›†åˆ/æ–‡ä»¶å¤¹",
            inputSchema={
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "é›†åˆåç§°"
                    },
                    "parent_key": {
                        "type": "string", 
                        "description": "çˆ¶é›†åˆkeyï¼ˆå¯é€‰ï¼Œç”¨äºåˆ›å»ºå­é›†åˆï¼‰"
                    }
                },
                "required": ["name"]
            }
        ),
        Tool(
            name="download_and_save_to_zotero",
            description="ä¸‹è½½è®ºæ–‡å¹¶ä¿å­˜åˆ°Zoteroï¼ˆä¸€æ­¥å®Œæˆï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "paper_url": {
                        "type": "string",
                        "description": "è®ºæ–‡URL"
                    },
                    "paper_title": {
                        "type": "string",
                        "description": "è®ºæ–‡æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰"
                    },
                    "collection_key": {
                        "type": "string",
                        "description": "ç›®æ ‡é›†åˆkeyï¼ˆå¯é€‰ï¼‰"
                    }
                },
                "required": ["paper_url"]
            }
        ),
        Tool(
            name="search_and_download",
            description="æœç´¢å¹¶è‡ªåŠ¨ä¸‹è½½ç¬¬ä¸€ç¯‡è®ºæ–‡ï¼ˆå¿«æ·æ“ä½œï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    },
                    "download_count": {
                        "type": "integer",
                        "description": "ä¸‹è½½è®ºæ–‡æ•°é‡ï¼ˆé»˜è®¤1ï¼‰",
                        "default": 1,
                        "minimum": 1,
                        "maximum": 5
                    }
                },
                "required": ["query"]
            }
        )
    ]
    return tools


@app.call_tool()
async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> list[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨"""
    global downloader
    
    try:
        if name == "set_cookies":
            cookies = arguments.get("cookies")
            
            if not cookies:
                return [TextContent(type="text", text="ç¼ºå°‘cookieså‚æ•°")]
            
            try:
                # åˆå§‹åŒ–ä¸‹è½½å™¨å¹¶è®¾ç½®cookies
                downloader = LightweightNatureDownloader(cookies)
                
                # æµ‹è¯•ç™»å½•çŠ¶æ€
                status = downloader.test_login_status()
                
                if status.get("logged_in"):
                    message = "âœ… Cookiesè®¾ç½®æˆåŠŸï¼Œå·²ç™»å½•Natureç½‘ç«™ï¼"
                else:
                    message = "âš ï¸ Cookieså·²è®¾ç½®ï¼Œä½†ç™»å½•çŠ¶æ€ä¸ç¡®å®šã€‚è¯·æ£€æŸ¥cookiesæ˜¯å¦æœ‰æ•ˆã€‚"
                
                message += f"\nğŸ“Š å½“å‰cookiesæ•°é‡: {status.get('cookies_count', 0)}"
                
                return [TextContent(type="text", text=message)]
                
            except Exception as e:
                return [TextContent(type="text", text=f"è®¾ç½®cookieså¤±è´¥: {str(e)}")]
        
        elif name == "load_cookies_from_browser":
            browser = arguments.get("browser", "chrome")
            
            try:
                if not downloader:
                    downloader = LightweightNatureDownloader()
                
                success = downloader.load_cookies_from_browser(browser)
                
                if success:
                    status = downloader.test_login_status()
                    message = f"âœ… æˆåŠŸä»{browser}æµè§ˆå™¨åŠ è½½cookiesï¼\n"
                    message += f"ç™»å½•çŠ¶æ€: {'å·²ç™»å½•' if status.get('logged_in') else 'æœªç™»å½•'}\n"
                    message += f"Cookiesæ•°é‡: {status.get('cookies_count', 0)}"
                else:
                    message = f"âŒ ä»{browser}æµè§ˆå™¨åŠ è½½cookieså¤±è´¥ã€‚\n"
                    message += "è¯·ç¡®ä¿:\n1. æµè§ˆå™¨å·²å…³é—­\n2. å·²åœ¨æµè§ˆå™¨ä¸­ç™»å½•Natureç½‘ç«™\n3. å®‰è£…äº†å¿…è¦çš„è§£å¯†åº“"
                
                return [TextContent(type="text", text=message)]
                
            except Exception as e:
                return [TextContent(type="text", text=f"ä»æµè§ˆå™¨åŠ è½½cookieså¤±è´¥: {str(e)}")]
        
        elif name == "test_login_status":
            if not downloader:
                return [TextContent(type="text", text="âŒ è¯·å…ˆè®¾ç½®cookiesæˆ–åˆå§‹åŒ–ä¸‹è½½å™¨")]
            
            status = downloader.test_login_status()
            
            if status.get("logged_in"):
                message = "âœ… å·²ç™»å½•Natureç½‘ç«™"
            else:
                message = "âŒ æœªç™»å½•æˆ–cookieså·²è¿‡æœŸ"
            
            message += f"\nğŸ“Š è¯¦ç»†ä¿¡æ¯:\n"
            message += f"  çŠ¶æ€ç : {status.get('status_code', 'N/A')}\n"
            message += f"  Cookiesæ•°é‡: {status.get('cookies_count', 0)}\n"
            message += f"  æµ‹è¯•URLçŠ¶æ€: {status.get('test_url_status', 'N/A')}"
            
            if status.get("error"):
                message += f"\nâŒ é”™è¯¯: {status['error']}"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "search_papers":
            if not downloader:
                return [TextContent(type="text", text="âŒ è¯·å…ˆè®¾ç½®cookies")]
            
            query = arguments.get("query")
            max_results = arguments.get("max_results", 10)
            
            if not query:
                return [TextContent(type="text", text="ç¼ºå°‘æœç´¢å…³é”®è¯")]
            
            papers = downloader.search_papers(query, max_results)
            
            if papers:
                result_text = f"ğŸ” æ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡:\n\n"
                for i, paper in enumerate(papers, 1):
                    result_text += f"{i}. **{paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}**\n"
                    result_text += f"   ä½œè€…: {paper.get('authors', 'æœªçŸ¥ä½œè€…')}\n"
                    result_text += f"   æœŸåˆŠ: {paper.get('journal', 'Nature')}\n"
                    result_text += f"   æ—¥æœŸ: {paper.get('date', 'æœªçŸ¥æ—¥æœŸ')}\n"
                    result_text += f"   é“¾æ¥: {paper.get('url', '')}\n"
                    if paper.get('abstract'):
                        abstract = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
                        result_text += f"   æ‘˜è¦: {abstract}\n"
                    result_text += "\n"
            else:
                result_text = f"âŒ æœªæ‰¾åˆ°å…³é”®è¯ '{query}' çš„ç›¸å…³è®ºæ–‡"
            
            return [TextContent(type="text", text=result_text)]
        
        elif name == "enhanced_search_papers":
            if not downloader:
                return [TextContent(type="text", text="âŒ è¯·å…ˆè®¾ç½®cookies")]
            
            keywords = arguments.get("keywords")
            max_results = arguments.get("max_results", 10)
            
            if not keywords:
                return [TextContent(type="text", text="ç¼ºå°‘æœç´¢å…³é”®è¯")]
            
            search_results = downloader.enhanced_search_papers(keywords, max_results)
            
            if search_results.get("error"):
                return [TextContent(type="text", text=f"âŒ æœç´¢å¤±è´¥: {search_results['error']}")]
            
            papers = search_results.get("papers", [])
            search_info = search_results.get("search_info", {})
            
            if papers:
                result_text = f"ğŸ¯ **å¢å¼ºæœç´¢ç»“æœ** - å…³é”®è¯: {keywords}\n"
                result_text += f"ğŸ“Š æœç´¢ç»Ÿè®¡: æ‰¾åˆ° {search_results.get('total_found', 0)} ç¯‡è®ºæ–‡ï¼Œè¿”å›å‰ {len(papers)} ç¯‡æœ€ç›¸å…³çš„\n"
                result_text += f"â° æœç´¢æ—¶é—´: {search_info.get('timestamp', 'æœªçŸ¥')}\n\n"
                
                for i, paper in enumerate(papers, 1):
                    relevance_score = paper.get('relevance_score', 0)
                    result_text += f"**{i}. {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}**\n"
                    result_text += f"   ğŸ¯ ç›¸å…³æ€§è¯„åˆ†: {relevance_score}/10\n"
                    result_text += f"   ğŸ‘¥ ä½œè€…: {paper.get('authors', 'æœªçŸ¥ä½œè€…')}\n"
                    result_text += f"   ğŸ“– æœŸåˆŠ: {paper.get('journal', 'Nature')}\n"
                    result_text += f"   ğŸ“… æ—¥æœŸ: {paper.get('date', 'æœªçŸ¥æ—¥æœŸ')}\n"
                    result_text += f"   ğŸ”— é“¾æ¥: {paper.get('url', '')}\n"
                    result_text += f"   ğŸ“¥ å¯ä¸‹è½½: {'âœ…' if paper.get('downloadable') else 'âŒ'}\n"
                    
                    if paper.get('abstract'):
                        abstract = paper['abstract'][:300] + "..." if len(paper['abstract']) > 300 else paper['abstract']
                        result_text += f"   ğŸ“ æ‘˜è¦: {abstract}\n"
                    
                    result_text += "\n"
                
                result_text += f"\nğŸ’¡ **ä½¿ç”¨æç¤º**: å¤åˆ¶è®ºæ–‡é“¾æ¥ï¼Œä½¿ç”¨ `download_paper` å·¥å…·è¿›è¡Œä¸‹è½½\n"
                result_text += f"ğŸ“‚ ä¸‹è½½ä½ç½®: {downloader.download_dir}"
                
            else:
                result_text = f"âŒ æœªæ‰¾åˆ°å…³é”®è¯ '{keywords}' çš„ç›¸å…³è®ºæ–‡\n"
                result_text += f"ğŸ’¡ å»ºè®®å°è¯•ä¸åŒçš„å…³é”®è¯æˆ–è€…æ›´é€šç”¨çš„æœç´¢è¯"
            
            return [TextContent(type="text", text=result_text)]
        
        elif name == "download_paper":
            if not downloader:
                return [TextContent(type="text", text="âŒ è¯·å…ˆè®¾ç½®cookies")]
            
            paper_url = arguments.get("paper_url")
            paper_title = arguments.get("paper_title", "")
            
            if not paper_url:
                return [TextContent(type="text", text="ç¼ºå°‘è®ºæ–‡URL")]
            
            result = downloader.download_paper(paper_url, paper_title)
            
            if result["success"]:
                message = f"âœ… {result['message']}\n"
                message += f"ä¸‹è½½çš„æ–‡ä»¶:\n"
                for file in result["files"]:
                    message += f"  ğŸ“„ {file}\n"
                message += f"\nğŸ“‚ æ–‡ä»¶ä¿å­˜ä½ç½®: {downloader.download_dir}"
            else:
                message = f"âŒ ä¸‹è½½å¤±è´¥: {result['message']}"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "download_papers_batch":
            if not downloader:
                return [TextContent(type="text", text="âŒ è¯·å…ˆè®¾ç½®cookies")]
            
            papers = arguments.get("papers", [])
            
            if not papers:
                return [TextContent(type="text", text="ç¼ºå°‘è®ºæ–‡åˆ—è¡¨")]
            
            # æ‰¹é‡ä¸‹è½½
            results = []
            success_count = 0
            total_files = 0
            
            for i, paper in enumerate(papers):
                url = paper.get("url", "")
                title = paper.get("title", f"è®ºæ–‡{i+1}")
                
                if not url:
                    continue
                
                result = downloader.download_paper(url, title)
                results.append(result)
                
                if result["success"]:
                    success_count += 1
                    total_files += len(result["files"])
            
            message = f"ğŸ“Š æ‰¹é‡ä¸‹è½½å®Œæˆ:\n"
            message += f"  âœ… æˆåŠŸ: {success_count}/{len(papers)} ç¯‡è®ºæ–‡\n"
            message += f"  ğŸ“„ å…±ä¸‹è½½: {total_files} ä¸ªæ–‡ä»¶\n\n"
            
            for i, result in enumerate(results, 1):
                paper_title = papers[i-1].get("title", f"è®ºæ–‡{i}")
                if result["success"]:
                    message += f"{i}. âœ… {paper_title}: {len(result['files'])} ä¸ªæ–‡ä»¶\n"
                else:
                    message += f"{i}. âŒ {paper_title}: {result['message']}\n"
            
            message += f"\nğŸ“‚ æ–‡ä»¶ä¿å­˜ä½ç½®: {downloader.download_dir}"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "get_cookies_guide":
            temp_downloader = LightweightNatureDownloader()
            guide = temp_downloader.get_cookies_from_browser_manual()
            
            return [TextContent(type="text", text=guide)]
        
        elif name == "export_cookies":
            if not downloader:
                return [TextContent(type="text", text="âŒ è¯·å…ˆè®¾ç½®cookies")]
            
            cookies_json = downloader.export_cookies()
            
            message = f"ğŸ“‹ å½“å‰cookiesï¼ˆJSONæ ¼å¼ï¼‰:\n\n```json\n{cookies_json}\n```\n\n"
            message += "ğŸ’¡ ä½ å¯ä»¥ä¿å­˜è¿™äº›cookiesï¼Œç¨åä½¿ç”¨ set_cookies å·¥å…·é‡æ–°å¯¼å…¥"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "search_and_download":
            if not downloader:
                return [TextContent(type="text", text="âŒ è¯·å…ˆè®¾ç½®cookies")]
            
            query = arguments.get("query")
            download_count = arguments.get("download_count", 1)
            
            if not query:
                return [TextContent(type="text", text="ç¼ºå°‘æœç´¢å…³é”®è¯")]
            
            # å…ˆæœç´¢
            papers = downloader.search_papers(query, download_count + 2)
            
            if not papers:
                return [TextContent(type="text", text=f"âŒ æœªæ‰¾åˆ°å…³é”®è¯ '{query}' çš„ç›¸å…³è®ºæ–‡")]
            
            # ä¸‹è½½å‰å‡ ç¯‡
            papers_to_download = papers[:download_count]
            results = []
            success_count = 0
            total_files = 0
            
            message = f"ğŸ” æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼Œæ­£åœ¨ä¸‹è½½å‰ {len(papers_to_download)} ç¯‡:\n\n"
            
            for i, paper in enumerate(papers_to_download):
                result = downloader.download_paper(paper.get("url", ""), paper.get("title", ""))
                results.append(result)
                
                if result["success"]:
                    success_count += 1
                    total_files += len(result["files"])
                    message += f"âœ… {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}: {len(result['files'])} ä¸ªæ–‡ä»¶\n"
                else:
                    message += f"âŒ {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}: {result['message']}\n"
            
            message += f"\nğŸ“Š æ€»ç»“:\n"
            message += f"  æˆåŠŸ: {success_count}/{len(papers_to_download)} ç¯‡è®ºæ–‡\n"
            message += f"  å…±ä¸‹è½½: {total_files} ä¸ªæ–‡ä»¶\n"
            message += f"  æ–‡ä»¶ä¿å­˜ä½ç½®: {downloader.download_dir}"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "check_zotero_status":
            if zotero_connector.is_running():
                version = zotero_connector.get_version() or "æœªçŸ¥ç‰ˆæœ¬"
                collections = zotero_connector.get_collections()
                collections_count = len(collections)
                
                message = f"âœ… **Zoteroè¿æ¥æ­£å¸¸**\n\n"
                message += f"ğŸ“Š **ç‰ˆæœ¬**: {version}\n"
                message += f"ğŸ“š **é›†åˆæ•°é‡**: {collections_count} ä¸ª\n"
                message += f"ğŸ”— **ç«¯å£**: {zotero_connector.port}\n"
                message += f"ğŸ—ƒï¸ **æ•°æ®åº“**: {'å·²æ‰¾åˆ°' if zotero_connector._zotero_db_path else 'æœªæ‰¾åˆ°'}\n\n"
                message += f"ğŸ¯ **å¯ç”¨åŠŸèƒ½**:\n"
                message += f"  âœ… `get_zotero_collections` - æŸ¥çœ‹æ‰€æœ‰é›†åˆ (æ•°æ®åº“ç›´è¯»)\n"
                message += f"  âœ… `save_to_zotero` - ä¿å­˜è®ºæ–‡å…ƒæ•°æ®\n"
                message += f"  âœ… `create_zotero_collection` - åˆ›å»ºæ–°é›†åˆ\n"
                message += f"  ğŸ”§ `download_and_save_to_zotero` - å®Œæ•´ä¸‹è½½ (éœ€cookies)\n\n"
                message += f"ğŸ‰ **çªç ´æ€§è¿›å±•**: ç°åœ¨å¯ä»¥ç›´æ¥è¯»å–Zoteroæ•°æ®åº“è·å–å®Œæ•´é›†åˆåˆ—è¡¨ï¼"
            else:
                message = f"âŒ **Zoteroä¸å¯ç”¨**\n\n"
                message += f"è¯·ç¡®ä¿:\n"
                message += f"  1. Zoteroæ¡Œé¢åº”ç”¨å·²å¯åŠ¨\n"
                message += f"  2. Zoteroç‰ˆæœ¬ä¸º6.0ä»¥ä¸Š\n"
                message += f"  3. æ²¡æœ‰é˜²ç«å¢™é˜»æ­¢æœ¬åœ°è¿æ¥(ç«¯å£23119)"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "get_zotero_collections":
            if not zotero_connector.is_running():
                return [TextContent(type="text", text="âŒ Zoteroä¸å¯ç”¨ï¼Œè¯·å¯åŠ¨Zoteroæ¡Œé¢åº”ç”¨")]
            
            collections = zotero_connector.get_collections()
            
            if collections and len(collections) > 0:
                # æ„å»ºå±‚çº§ç»“æ„çš„è¾…åŠ©å‡½æ•°
                def build_tree_display(collections_list):
                    # æŒ‰å±‚çº§ç»„ç»‡
                    root_collections = [c for c in collections_list if not c.get('parentCollection')]
                    child_collections = [c for c in collections_list if c.get('parentCollection')]
                    
                    # åˆ›å»ºçˆ¶å­å…³ç³»æ˜ å°„
                    children_map = {}
                    for child in child_collections:
                        parent_id = child.get('parentCollection')
                        if parent_id not in children_map:
                            children_map[parent_id] = []
                        children_map[parent_id].append(child)
                    
                    tree_text = ""
                    
                    def add_collection(collection, level=0, is_last=False, parent_prefix=""):
                        nonlocal tree_text
                        
                        name = collection.get('name', 'æœªå‘½åé›†åˆ')
                        key = collection.get('key', 'N/A')
                        
                        # æ ¹æ®å±‚çº§ç¡®å®šå‰ç¼€
                        if level == 0:
                            prefix = ""
                            line_prefix = ""
                        else:
                            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                            prefix = parent_prefix + connector
                            line_prefix = parent_prefix + ("    " if is_last else "â”‚   ")
                        
                        # æ·»åŠ é›†åˆä¿¡æ¯
                        tree_text += f"{prefix}ğŸ“ **{name}**\n"
                        tree_text += f"{line_prefix}ğŸ”‘ `{key}`\n"
                        
                        # æ·»åŠ å­é›†åˆ
                        collection_id = collection.get('id')
                        if collection_id in children_map:
                            children = children_map[collection_id]
                            for i, child in enumerate(children):
                                is_last_child = (i == len(children) - 1)
                                add_collection(child, level + 1, is_last_child, line_prefix)
                        
                        if level == 0:  # æ ¹é›†åˆä¹‹é—´æ·»åŠ ç©ºè¡Œ
                            tree_text += "\n"
                    
                    # æ·»åŠ æ‰€æœ‰æ ¹é›†åˆ
                    for root in sorted(root_collections, key=lambda x: x.get('name', '')):
                        add_collection(root)
                    
                    # å¤„ç†å­¤ç«‹çš„å­é›†åˆï¼ˆæ²¡æœ‰å¯¹åº”çˆ¶é›†åˆçš„ï¼‰
                    orphans = [c for c in child_collections 
                             if not any(p.get('id') == c.get('parentCollection') for p in root_collections)]
                    
                    if orphans:
                        tree_text += "**ğŸ” å­¤ç«‹é›†åˆ** (å¯èƒ½çˆ¶é›†åˆå·²åˆ é™¤):\n"
                        for orphan in orphans:
                            add_collection(orphan)
                    
                    return tree_text
                
                message = f"ğŸ“š **Zoteroé›†åˆæ ‘çŠ¶ç»“æ„** ({len(collections)} ä¸ªé›†åˆ):\n\n"
                message += build_tree_display(collections)
                
                message += f"ğŸ’¡ **ä½¿ç”¨è¯´æ˜**:\n"
                message += f"â€¢ å¤åˆ¶ä¸Šé¢çš„ğŸ”‘Keyå€¼ç”¨ä½œ `collection_key` å‚æ•°\n"
                message += f"â€¢ ä¸æŒ‡å®šcollection_key â†’ ä¿å­˜åˆ°ğŸ“šæˆ‘çš„æ–‡åº“ï¼ˆé»˜è®¤ä½ç½®ï¼‰\n"
                message += f"â€¢ æŒ‡å®šcollection_key â†’ ä¿å­˜åˆ°ğŸ¯æŒ‡å®šé›†åˆ\n"
                message += f"â€¢ å±‚çº§ç»“æ„ï¼šâ””â”€â”€ è¡¨ç¤ºå­é›†åˆï¼Œâ”œâ”€â”€ è¡¨ç¤ºæœ‰å…„å¼Ÿé›†åˆ"
            else:
                message = f"ğŸ“š **Zoteroä¸­æš‚æ— é›†åˆ**\n\n"
                message += f"çœ‹èµ·æ¥ä½ çš„Zoteroä¸­è¿˜æ²¡æœ‰åˆ›å»ºä»»ä½•é›†åˆï¼ˆæ–‡ä»¶å¤¹ï¼‰ã€‚\n\n"
                message += f"**å»ºè®®**:\n"
                message += f"â€¢ ğŸ“ åœ¨Zoteroä¸­æ‰‹åŠ¨åˆ›å»ºä¸€äº›é›†åˆ\n"
                message += f"â€¢ â• ä½¿ç”¨ `create_zotero_collection` å·¥å…·åˆ›å»ºæ–°é›†åˆ\n"
                message += f"â€¢ ğŸ’¾ ç›´æ¥ä½¿ç”¨ `save_to_zotero` ä¿å­˜è®ºæ–‡ï¼ˆä¼šä¿å­˜åˆ°é»˜è®¤ä½ç½®ï¼‰"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "save_to_zotero":
            paper_url = arguments.get("paper_url")
            paper_title = arguments.get("paper_title", "")
            collection_key = arguments.get("collection_key")
            
            if not paper_url:
                return [TextContent(type="text", text="ç¼ºå°‘è®ºæ–‡URL")]
            
            if not zotero_connector.is_running():
                return [TextContent(type="text", text="âŒ Zoteroä¸å¯ç”¨ï¼Œè¯·å¯åŠ¨Zoteroæ¡Œé¢åº”ç”¨")]
            
            # æ„å»ºåŸºæœ¬è®ºæ–‡ä¿¡æ¯
            paper_info = {
                "title": paper_title,
                "url": paper_url,
                "journal": "Nature",
                "itemType": "journalArticle"
            }
            
            # æ˜¾ç¤ºå¤„ç†è¿›åº¦
            if 'arxiv.org' in paper_url:
                processing_msg = "ğŸ”„ å¤„ç†arxivè®ºæ–‡...\n"
                processing_msg += "â€¢ æå–è®ºæ–‡å…ƒæ•°æ®\n"
                processing_msg += "â€¢ è·å–ä½œè€…ã€æ‘˜è¦ã€æ—¥æœŸç­‰ä¿¡æ¯\n"
                processing_msg += "â€¢ ä¿å­˜åˆ°Zotero...\n"
                logger.info("å¼€å§‹å¤„ç†arxivè®ºæ–‡")
            
            result = zotero_connector.save_item_to_zotero(paper_info, collection_key=collection_key)
            
            if result["success"]:
                message = f"ğŸ‰ **æˆåŠŸä¿å­˜åˆ°Zotero!**\n\n"
                
                # æ˜¾ç¤ºä½¿ç”¨çš„æ•°æ®åº“ - ä¼˜å…ˆä½¿ç”¨çœŸå®æ•°æ®åº“åç§°
                source = result.get("source", "")
                database = result.get("database", "æœªçŸ¥")
                
                # ğŸ¯ ä¼˜å…ˆæ˜¾ç¤ºçœŸå®æ•°æ®åº“åç§°è€Œéæå–å™¨åç§°
                display_database = source or database
                if display_database == "Browser-Driven":
                    display_database = "æœªçŸ¥æ•°æ®åº“"
                
                enhanced = result.get("enhanced", False)
                
                message += f"ğŸ”— **æ•°æ®åº“**: {display_database}\n"
                message += f"ğŸ¤– **æ™ºèƒ½å¢å¼º**: {'âœ… æ˜¯' if enhanced else 'â– å¦'}\n\n"
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯arxivè®ºæ–‡å¹¶æä¾›æ›´è¯¦ç»†ä¿¡æ¯
                if 'arxiv.org' in paper_url:
                    message += f"ğŸ“„ **è®ºæ–‡ç±»å‹**: arXivé¢„å°æœ¬\n"
                    
                    # å°è¯•ä»URLæå–arxiv ID
                    import re
                    arxiv_match = re.search(r'arxiv\.org/(abs|pdf)/([^/?]+)', paper_url)
                    if arxiv_match:
                        arxiv_id = arxiv_match.group(2)
                        message += f"ğŸ·ï¸ **arXiv ID**: {arxiv_id}\n"
                        # ğŸ¯ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨è¿”å›ç»“æœä¸­çš„æ ‡é¢˜
                        actual_title = result.get('title') or paper_title or f'arXiv:{arxiv_id} (æ ‡é¢˜æå–ä¸­...)'
                        message += f"ğŸ“„ **æ ‡é¢˜**: {actual_title}\n"
                        message += f"ğŸ”— **æ‘˜è¦é¡µé¢**: https://arxiv.org/abs/{arxiv_id}\n"
                        message += f"ğŸ“¥ **PDFé“¾æ¥**: https://arxiv.org/pdf/{arxiv_id}.pdf\n"
                else:
                    # ğŸ¯ æ ¹æ®itemTypeå’Œsourceç¡®å®šè®ºæ–‡ç±»å‹
                    item_type = result.get('itemType', '')
                    paper_type = "æœŸåˆŠæ–‡ç« "
                    
                    if item_type == 'preprint' or source in ['bioRxiv', 'medRxiv', 'ChemRxiv', 'PsyArXiv']:
                        paper_type = "é¢„å°æœ¬"
                    elif source:
                        paper_type = f"{source}è®ºæ–‡"
                    elif display_database != "æœªçŸ¥æ•°æ®åº“":
                        paper_type = f"{display_database}æœŸåˆŠæ–‡ç« "
                    
                    message += f"ğŸ“„ **è®ºæ–‡ç±»å‹**: {paper_type}\n"
                    # ğŸ¯ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨è¿”å›ç»“æœä¸­çš„æ ‡é¢˜ï¼Œè€Œéç©ºçš„paper_title
                    actual_title = result.get('title') or paper_title or 'æ ‡é¢˜æå–ä¸­...'
                    message += f"ğŸ“„ **æ ‡é¢˜**: {actual_title}\n"
                    message += f"ğŸ”— **URL**: {paper_url}\n"
                
                # é›†åˆä¿å­˜çŠ¶æ€
                if collection_key:
                    if result.get("collection_specified"):
                        message += f"âœ… **ä¿å­˜é›†åˆ**: å·²æŒ‡å®šåˆ° {collection_key}\n"
                    else:
                        message += f"âš ï¸ **ä¿å­˜é›†åˆ**: æŒ‡å®šå¤±è´¥ï¼Œå·²ä¿å­˜åˆ°é»˜è®¤ä½ç½®\n"
                else:
                    message += f"ğŸ“š **ä¿å­˜ä½ç½®**: æˆ‘çš„æ–‡åº“ï¼ˆé»˜è®¤ä½ç½®ï¼‰\n"
                
                # updateSessionä¿®å¤çŠ¶æ€
                if result.get("update_session_used"):
                    message += f"\nğŸ“ **é›†åˆç§»åŠ¨çŠ¶æ€** (åŸºäºå®˜æ–¹æºç ):\n"
                    
                    if result.get("collection_move_success"):
                        message += f"ğŸ‰ **é›†åˆç§»åŠ¨**: âœ… æˆåŠŸä½¿ç”¨updateSessionç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ\n"
                        message += f"âœ¨ **é‡å¤§çªç ´**: åŸºäºZotero Connectorå®˜æ–¹updateSessionæœºåˆ¶\n"
                    else:
                        message += f"âš ï¸ **é›†åˆç§»åŠ¨**: âŒ updateSessionå¤±è´¥ï¼Œæ¡ç›®åœ¨é»˜è®¤ä½ç½®\n"
                    
                    if result.get("pdf_success"):
                        message += f"âœ… **PDFé™„ä»¶**: å·²è‡ªåŠ¨ä¸‹è½½å¹¶é™„åŠ \n"
                    
                    if result.get("extra_preserved"):
                        message += f"âœ… **Commentä¿¡æ¯**: å·²ä¿å­˜åœ¨Extraå­—æ®µä¸­\n"
                
                if result.get("zotero_url"):
                    message += f"ğŸš€ **Zoteroé“¾æ¥**: {result['zotero_url']}\n"
                
                message += f"\nâœ¨ **å·²æå–çš„å…ƒæ•°æ®**:\n"
                message += f"â€¢ ğŸ” å®Œæ•´è®ºæ–‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€æ—¥æœŸï¼‰\n"
                message += f"â€¢ ğŸ“„ é¡µæ•°å’Œå›¾è¡¨ä¿¡æ¯ï¼ˆ\"15 pages, 5 figures\"ï¼‰\n"
                message += f"â€¢ ğŸ“š å­¦ç§‘åˆ†ç±»ä¿¡æ¯\n"
                message += f"â€¢ ğŸ·ï¸ æ­£ç¡®çš„æ–‡çŒ®ç±»å‹ï¼ˆé¢„å°æœ¬/æœŸåˆŠæ–‡ç« ï¼‰\n"
                message += f"â€¢ ğŸ”— å®Œæ•´çš„å¼•ç”¨ä¿¡æ¯\n"
                
                message += f"\nğŸ“‹ **ç«‹å³éªŒè¯**:\n"
                if result.get("collection_move_success"):
                    message += f"ğŸ‰ **ç»ˆææˆåŠŸï¼updateSessioné›†åˆç§»åŠ¨**:\n"
                    message += f"1. æ‰“å¼€Zoteroæ¡Œé¢åº”ç”¨\n"
                    message += f"2. **ç›´æ¥æ£€æŸ¥My Researché›†åˆ**\n"
                    message += f"3. æ¡ç›®åº”è¯¥å·²ç»åœ¨æ­£ç¡®çš„é›†åˆä¸­äº†ï¼\n"
                    if result.get("pdf_success"):
                        message += f"4. ç¡®è®¤PDFé™„ä»¶å·²è‡ªåŠ¨æ·»åŠ \n"
                    if result.get("extra_preserved"):
                        message += f"5. æŸ¥çœ‹Extraå­—æ®µçš„Commentä¿¡æ¯\n"
                    message += f"\nğŸ¯ **è¿™æ˜¯çœŸæ­£çš„å®Œå…¨è‡ªåŠ¨åŒ–ï¼**\n"
                elif result.get("update_session_used"):
                    message += f"âš ï¸ **updateSessionå°è¯•å¤±è´¥**:\n"
                    message += f"1. æ‰“å¼€Zoteroæ¡Œé¢åº”ç”¨\n"
                    message += f"2. åœ¨\"æˆ‘çš„æ–‡åº“\"ä¸­æ‰¾åˆ°æ¡ç›®\n"
                    message += f"3. æ‰‹åŠ¨æ‹–æ‹½åˆ°My Researché›†åˆ\n"
                else:
                    message += f"ğŸ“š æ‰“å¼€Zoteroæ¡Œé¢åº”ç”¨æŸ¥çœ‹ä¿å­˜çš„æ¡ç›®\n"
            else:
                message = f"âŒ **ä¿å­˜å¤±è´¥**: {result.get('message', 'æœªçŸ¥é”™è¯¯')}\n\n"
                message += f"ğŸ”§ **æ•…éšœæ’é™¤**:\n"
                message += f"â€¢ ç¡®ä¿Zoteroæ¡Œé¢åº”ç”¨æ­£åœ¨è¿è¡Œ\n"
                message += f"â€¢ æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                message += f"â€¢ å°è¯•é‡æ–°å¯åŠ¨Zoteroåº”ç”¨\n"
                message += f"â€¢ å¦‚æœæ˜¯é›†åˆé—®é¢˜ï¼Œå¯å°è¯•ä¸æŒ‡å®šcollection_key"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "create_zotero_collection":
            collection_name = arguments.get("name", "").strip()
            parent_key = arguments.get("parent_key", "").strip() or None
            
            if not collection_name:
                return [TextContent(type="text", text="ç¼ºå°‘é›†åˆåç§°")]
            
            if not zotero_connector.is_running():
                return [TextContent(type="text", text="âŒ Zoteroä¸å¯ç”¨ï¼Œè¯·å¯åŠ¨Zoteroæ¡Œé¢åº”ç”¨")]
            
            # ç”±äºZotero Connector APIé™åˆ¶ï¼Œæä¾›æ‰‹åŠ¨åˆ›å»ºæŒ‡å¯¼
            message = f"âš ï¸ **Zotero APIé™åˆ¶è¯´æ˜**\n\n"
            message += f"å¾ˆæŠ±æ­‰ï¼ŒZoteroçš„æœ¬åœ°Connector APIä¸æ”¯æŒé€šè¿‡ä»£ç åˆ›å»ºé›†åˆã€‚\n"
            message += f"è¿™æ˜¯Zoteroè½¯ä»¶çš„è®¾è®¡é™åˆ¶ã€‚\n\n"
            message += f"ğŸ¯ **è¯·æ‰‹åŠ¨åˆ›å»ºé›†åˆ**ï¼š\n"
            message += f"1. ğŸ“± æ‰“å¼€ä½ çš„**Zoteroæ¡Œé¢åº”ç”¨**\n"
            message += f"2. ğŸ–±ï¸ å³é”®ç‚¹å‡»å·¦ä¾§æ–‡ä»¶å¤¹åŒºåŸŸ\n"
            message += f"3. â• é€‰æ‹© **\"æ–°å»ºé›†åˆ\"**\n"
            message += f"4. ğŸ“ è¾“å…¥é›†åˆåç§°ï¼š**{collection_name}**\n"
            message += f"5. âœ… ç¡®è®¤åˆ›å»º\n\n"
            message += f"ğŸ“š **åˆ›å»ºå®Œæˆå**ï¼š\n"
            message += f"â€¢ ä½¿ç”¨ `get_zotero_collections` å·¥å…·è·å–æ–°é›†åˆçš„Key\n"
            message += f"â€¢ ä½¿ç”¨è·å–åˆ°çš„Keyä¿å­˜è®ºæ–‡åˆ°æŒ‡å®šé›†åˆ\n"
            message += f"â€¢ æˆ–è€…ç›´æ¥ä½¿ç”¨ `save_to_zotero`ï¼ˆä¸æŒ‡å®šcollection_keyï¼‰ä¿å­˜åˆ°é»˜è®¤ä½ç½®\n\n"
            message += f"ğŸ’¡ **æç¤º**: æ‰‹åŠ¨åˆ›å»ºé›†åˆåªéœ€è¦10ç§’é’Ÿï¼Œä¹‹åå°±å¯ä»¥æ­£å¸¸ä½¿ç”¨æ‰€æœ‰ä¿å­˜åŠŸèƒ½äº†ï¼"
            
            return [TextContent(type="text", text=message)]
        
        elif name == "download_and_save_to_zotero":
            paper_url = arguments.get("paper_url")
            paper_title = arguments.get("paper_title", "")
            collection_key = arguments.get("collection_key")
            
            if not paper_url:
                return [TextContent(type="text", text="ç¼ºå°‘è®ºæ–‡URL")]
            
            if not zotero_connector.is_running():
                return [TextContent(type="text", text="âŒ Zoteroä¸å¯ç”¨ï¼Œè¯·å¯åŠ¨Zoteroæ¡Œé¢åº”ç”¨")]
            
            # æ­¤å·¥å…·éœ€è¦ä¸‹è½½åŠŸèƒ½ï¼Œæ‰€ä»¥ç¡®å®éœ€è¦cookies
            if not downloader:
                return [TextContent(type="text", text="âŒ ä¸‹è½½åŠŸèƒ½éœ€è¦å…ˆè®¾ç½®cookies\nğŸ’¡ ä½ å¯ä»¥ä½¿ç”¨ `save_to_zotero` ä»…ä¿å­˜å…ƒæ•°æ®ï¼ˆæ— éœ€cookiesï¼‰")]
            
            result = downloader.download_and_save_to_zotero(paper_url, paper_title, collection_key)
            
            if result["success"]:
                message = f"ğŸ‰ æˆåŠŸä¸‹è½½å¹¶ä¿å­˜åˆ°Zotero!\n\n"
                message += f"ğŸ“„ æ ‡é¢˜: {result.get('title', paper_title)}\n"
                message += f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶:\n"
                for file_path in result.get("download_files", []):
                    message += f"  â€¢ {file_path}\n"
                
                if collection_key:
                    message += f"ğŸ“š ä¿å­˜åˆ°é›†åˆ: {collection_key}\n"
                
                if result.get("zotero_url"):
                    message += f"ğŸš€ åœ¨Zoteroä¸­æ‰“å¼€: {result['zotero_url']}\n"
                
                message += f"\nâœ¨ è®ºæ–‡å·²åŒæ—¶ä¿å­˜åˆ°:\n"
                message += f"  â€¢ æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ: {downloader.download_dir}\n"
                message += f"  â€¢ Zoteroæ•°æ®åº“ (åŒ…å«PDFé™„ä»¶)"
            else:
                message = f"âŒ æ“ä½œå¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            
            return [TextContent(type="text", text=message)]
        
        else:
            return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]
    
    except Exception as e:
        logger.error(f"å·¥å…·è°ƒç”¨æ—¶å‡ºé”™ {name}: {str(e)}")
        return [TextContent(type="text", text=f"å·¥å…·è°ƒç”¨æ—¶å‡ºé”™: {str(e)}")]


async def main():
    """è¿è¡ŒMCPæœåŠ¡å™¨"""
    # è®¾ç½®æœåŠ¡å™¨é€‰é¡¹
    options = InitializationOptions(
        server_name="nature-scholar-tool",
        server_version="0.2.0",
        capabilities={
            "resources": {},
            "tools": {}
        }
    )
    
    async with stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            options
        )


if __name__ == "__main__":
    asyncio.run(main()) 