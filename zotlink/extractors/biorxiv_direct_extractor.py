#!/usr/bin/env python3
"""
üß¨ BioRxiv‰∏ìÁî®Áõ¥Êé•ÊèêÂèñÂô®
‰ΩøÁî®È™åËØÅÊàêÂäüÁöÑMCPÊµèËßàÂô®ÊäÄÊúØ
"""

import asyncio
import tempfile
import os
import re
import logging
from typing import Dict, Any, Optional
from pathlib import Path
from .base_extractor import BaseExtractor

logger = logging.getLogger(__name__)

class BioRxivDirectExtractor(BaseExtractor):
    """BioRxiv‰∏ìÁî®ÊèêÂèñÂô® - Â∑≤È™åËØÅÂèØÁªïËøáÂèçÁà¨Ëô´"""
    
    def __init__(self, session=None):
        super().__init__(session)
    
    def get_database_name(self) -> str:
        """ËøîÂõûÊï∞ÊçÆÂ∫ìÂêçÁß∞"""
        return "bioRxiv"
    
    def requires_authentication(self) -> bool:
        """ËøîÂõûÊòØÂê¶ÈúÄË¶ÅËÆ§ËØÅ"""
        return False
    
    def can_handle(self, url: str) -> bool:
        """Ê£ÄÊü•ÊòØÂê¶ÊòØbioRxiv URL"""
        return 'biorxiv.org' in url.lower()
    
    def extract_metadata(self, url: str) -> Dict[str, Any]:
        """ÊèêÂèñbioRxivËÆ∫ÊñáÂÖÉÊï∞ÊçÆÂíåPDFÂÜÖÂÆπ"""
        if not self.can_handle(url):
            return {}
        
        logger.info(f"üß¨ ‰ΩøÁî®BioRxiv‰∏ìÁî®ÊèêÂèñÂô®: {url}")
        
        # ‰ªéURLÊèêÂèñÂü∫Êú¨‰ø°ÊÅØ
        basic_info = self._extract_from_url(url)
        
        # ‰∏ãËΩΩPDFÂÜÖÂÆπ
        pdf_content = self._download_pdf_content(basic_info['pdf_url'])
        
        if pdf_content:
            basic_info['pdf_content'] = pdf_content
            basic_info['pdf_size'] = len(pdf_content)
            logger.info(f"‚úÖ BioRxiv PDF‰∏ãËΩΩÊàêÂäü: {len(pdf_content):,} bytes")
        else:
            logger.warning("‚ö†Ô∏è BioRxiv PDF‰∏ãËΩΩÂ§±Ë¥•")
        
        return basic_info
    
    def _extract_from_url(self, url: str) -> Dict[str, Any]:
        """‰ªéURLÊèêÂèñÂü∫Êú¨‰ø°ÊÅØ"""
        # ÊèêÂèñDOIÂíåÊó•Êúü
        doi_match = re.search(r'10\.1101/(\d{4})\.(\d{2})\.(\d{2})\.(\d+)', url)
        if not doi_match:
            return {"error": "Êó†Ê≥ï‰ªéURLÊèêÂèñDOI"}
        
        year, month, day, version = doi_match.groups()
        doi = f"10.1101/{year}.{month}.{day}.{version}"
        paper_id = f"{year}.{month}.{day}.{version}"
        
        # ÊûÑÈÄ†ÂÖÉÊï∞ÊçÆ
        metadata = {
            "itemType": "preprint", 
            "title": f"bioRxiv preprint {paper_id}",
            "creators": [{"creatorType": "author", "firstName": "Unknown", "lastName": "Author"}],
            "abstractNote": "bioRxiv preprint - PDF auto-downloaded",
            "url": url,
            "DOI": doi,
            "repository": "bioRxiv", 
            "archiveID": paper_id,
            "date": f"{year}-{month}-{day}",
            "libraryCatalog": "bioRxiv",
            "pdf_url": f"https://www.biorxiv.org/content/10.1101/{paper_id}.full.pdf",
            "extractor": "BioRxiv-Direct"
        }
        
        return metadata
    
    def _download_pdf_content(self, pdf_url: str) -> Optional[bytes]:
        """‰∏ãËΩΩPDFÂÜÖÂÆπÔºàÂú®Êñ∞Á∫øÁ®ã‰∏≠ÊâßË°åÂºÇÊ≠•‰ªªÂä°Ôºâ"""
        try:
            import concurrent.futures
            
            def download_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(self._async_download_pdf(pdf_url))
                finally:
                    new_loop.close()
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(download_in_thread)
                return future.result(timeout=60)
                
        except Exception as e:
            logger.error(f"‚ùå PDF‰∏ãËΩΩÁ∫øÁ®ãÂºÇÂ∏∏: {e}")
            return None
    
    async def _async_download_pdf(self, pdf_url: str) -> Optional[bytes]:
        """ÂºÇÊ≠•‰∏ãËΩΩPDF - ‰ΩøÁî®È™åËØÅÊàêÂäüÁöÑMCPÊñπÊ≥ï"""
        from playwright.async_api import async_playwright
        
        playwright = await async_playwright().start()
        browser = await playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox', 
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-web-security',
                '--allow-running-insecure-content',
                '--disable-features=TranslateUI',
                '--no-first-run',
                '--no-default-browser-check'
            ]
        )
        
        context = await browser.new_context(
            viewport={'width': 1366, 'height': 768},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'DNT': '1'
            }
        )
        
        page = await context.new_page()
        
        # ÂèçÊ£ÄÊµãËÑöÊú¨
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            Object.defineProperty(navigator, 'plugins', {get: () => [{name: 'Chrome PDF Plugin'}]});
            Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});
            delete Object.getPrototypeOf(navigator).webdriver;
        """)
        
        try:
            download_success = False
            pdf_content = None
            
            async def handle_download(download):
                nonlocal download_success, pdf_content
                try:
                    temp_file = tempfile.NamedTemporaryFile(suffix='.pdf', delete=False)
                    temp_path = temp_file.name
                    temp_file.close()
                    
                    await download.save_as(temp_path)
                    
                    with open(temp_path, 'rb') as f:
                        pdf_content = f.read()
                    
                    if pdf_content and pdf_content.startswith(b'%PDF'):
                        download_success = True
                        logger.info(f"‚úÖ PDF‰∏ãËΩΩÊàêÂäü: {len(pdf_content):,} bytes")
                    
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è PDF‰∏ãËΩΩÂ§ÑÁêÜÂºÇÂ∏∏: {e}")
            
            page.on("download", handle_download)
            
            # Â§öÊ≠•È™§ËÆøÈóÆ
            await page.goto('https://www.biorxiv.org/', wait_until='networkidle', timeout=20000)
            await asyncio.sleep(2)
            
            # Ëß¶Âèë‰∏ãËΩΩ
            try:
                await page.evaluate(f"window.open('{pdf_url}', '_blank')")
            except:
                pass
            
            # Á≠âÂæÖ‰∏ãËΩΩ
            for i in range(30):
                if download_success:
                    break
                await asyncio.sleep(1)
            
            return pdf_content
            
        finally:
            await context.close()
            await browser.close()
            await playwright.stop() 