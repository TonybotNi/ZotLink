#!/usr/bin/env python3
"""
ğŸ§ª é¢„å°æœ¬ç½‘ç«™æ ‡é¢˜æå–æµ‹è¯•è„šæœ¬
æµ‹è¯•bioRxivã€medRxivã€chemRxivçš„æ ‡é¢˜æå–èƒ½åŠ›
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "zotlink"))

from zotlink.extractors.extractor_manager import ExtractorManager
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

async def test_url_extraction(url: str, expected_title_keywords: list = None):
    """æµ‹è¯•å•ä¸ªURLçš„æå–æ•ˆæœ"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª æµ‹è¯•URL: {url}")
    print(f"{'='*80}")
    
    try:
        manager = ExtractorManager()
        result = await manager.extract_metadata(url)
        
        if result:
            print(f"âœ… æå–æˆåŠŸï¼")
            print(f"ğŸ“„ æ ‡é¢˜: {result.get('title', 'æœªæå–åˆ°')}")
            print(f"ğŸ”§ æå–å™¨: {result.get('extractor', 'æœªçŸ¥')}")
            print(f"ğŸ“Š å­—æ®µæ•°é‡: {len(result)} ä¸ª")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸå…³é”®è¯
            title = result.get('title', '')
            if expected_title_keywords and title:
                found_keywords = [kw for kw in expected_title_keywords if kw.lower() in title.lower()]
                if found_keywords:
                    print(f"âœ… æ ‡é¢˜åŒ…å«é¢„æœŸå…³é”®è¯: {found_keywords}")
                else:
                    print(f"âš ï¸ æ ‡é¢˜å¯èƒ½ä¸å‡†ç¡®ï¼Œæœªæ‰¾åˆ°é¢„æœŸå…³é”®è¯: {expected_title_keywords}")
            
            # æ˜¾ç¤ºå…¶ä»–é‡è¦å­—æ®µ
            for field in ['authors', 'DOI', 'pdf_url', 'abstractNote']:
                if result.get(field):
                    value = result[field]
                    if isinstance(value, str) and len(value) > 50:
                        value = value[:50] + "..."
                    print(f"  {field}: {value}")
        else:
            print(f"âŒ æå–å¤±è´¥æˆ–è¿”å›ç©ºç»“æœ")
            
    except Exception as e:
        print(f"âŒ æå–å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é¢„å°æœ¬ç½‘ç«™æ ‡é¢˜æå–æµ‹è¯•")
    
    # æµ‹è¯•URLåˆ—è¡¨
    test_cases = [
        {
            'url': 'https://www.biorxiv.org/content/10.1101/2025.09.24.677899v1',
            'expected_keywords': ['bioRxiv', 'preprint']  # ç”¨æˆ·æä¾›çš„æµ‹è¯•URL
        },
        {
            'url': 'https://www.medrxiv.org/content/10.1101/2025.09.22.25336422v1',
            'expected_keywords': ['medRxiv']
        },
        # å¦‚æœæœ‰chemRxivçš„URLï¼Œå¯ä»¥æ·»åŠ æµ‹è¯•
    ]
    
    for test_case in test_cases:
        await test_url_extraction(test_case['url'], test_case.get('expected_keywords'))
        await asyncio.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    print(f"\n{'='*80}")
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹ä¸Šè¿°ç»“æœï¼Œç¡®è®¤æ ‡é¢˜æå–æ˜¯å¦æˆåŠŸã€‚")
    print("å¦‚æœæ ‡é¢˜ä»ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ç½‘ç«™çš„HTMLç»“æ„æ˜¯å¦å‘ç”Ÿå˜åŒ–ã€‚")
    print(f"{'='*80}")

if __name__ == "__main__":
    asyncio.run(main())
